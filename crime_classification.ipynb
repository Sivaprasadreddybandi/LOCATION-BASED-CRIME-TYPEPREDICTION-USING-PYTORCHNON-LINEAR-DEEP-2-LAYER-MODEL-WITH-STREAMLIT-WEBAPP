{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anky/Documents/crime_prediction/preprocess_data.py\", line 5, in <module>\n",
      "    data_frame_main = pd.read_csv(\"data.csv\")\n",
      "  File \"/home/anky/.local/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/anky/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 680, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/home/anky/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 575, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/home/anky/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 934, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"/home/anky/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1218, in _make_engine\n",
      "    self.handles = get_handle(  # type: ignore[call-overload]\n",
      "  File \"/home/anky/.local/lib/python3.10/site-packages/pandas/io/common.py\", line 786, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data.csv'\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2067, 11)\n",
      "(2067, 6)\n"
     ]
    }
   ],
   "source": [
    "data_frame_main  = pd.read_csv(\"main_data.csv\")\n",
    "\n",
    "inputs = data_frame_main[['month','day','hour','dayofyear','week','weekofyear','dayofweek','weekday','quarter','latitude','longitude']]\n",
    "targets = data_frame_main[['act379','act13','act279','act323','act363','act302']]\n",
    "\n",
    "\n",
    "inputs_ar = np.array(inputs)\n",
    "targets_ar = np.array(targets)\n",
    "\n",
    "print(inputs.shape)\n",
    "\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = data_frame_main['year']\n",
    "y = data_frame_main['act279']\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_frame_main['year'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_samples,no_features = inputs_ar.shape\n",
    "no_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train,inputs_test,targets_train,targets_test = train_test_split(inputs_ar, targets_ar, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = torch.from_numpy(inputs_train.astype(np.float32))\n",
    "inputs_test = torch.from_numpy(inputs_test.astype(np.float32))\n",
    "targets_train = torch.from_numpy(targets_train.astype(np.float32))\n",
    "targets_test = torch.from_numpy(targets_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1653, 11])\n",
      "torch.Size([414, 11])\n",
      "torch.Size([1653, 6])\n",
      "torch.Size([414, 6])\n"
     ]
    }
   ],
   "source": [
    "# targets_train = targets_train.reshape(-1,1)\n",
    "# targets_test = targets_test.reshape(-1,1)\n",
    "print(inputs_train.shape)\n",
    "print(inputs_test.shape)\n",
    "print(targets_train.shape)\n",
    "print(targets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,no_of_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(no_of_features,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(10,6)\n",
    "        \n",
    "\n",
    "    def forward(self, targets_train ):\n",
    "        out = self.linear1(targets_train)\n",
    "        out  = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(no_of_features=no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear1): Linear(in_features=11, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=10, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1500*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 60.6170, ACC: 0.45169082283973694\n",
      "Epoch: 20, Loss: 56.8374, ACC: 0.3840579688549042\n",
      "Epoch: 30, Loss: 53.2638, ACC: 0.36231884360313416\n",
      "Epoch: 40, Loss: 49.9021, ACC: 0.35748791694641113\n",
      "Epoch: 50, Loss: 46.7511, ACC: 0.35748791694641113\n",
      "Epoch: 60, Loss: 43.8046, ACC: 0.36231884360313416\n",
      "Epoch: 70, Loss: 41.0563, ACC: 0.3478260934352875\n",
      "Epoch: 80, Loss: 38.4860, ACC: 0.3478260934352875\n",
      "Epoch: 90, Loss: 36.0678, ACC: 0.32125604152679443\n",
      "Epoch: 100, Loss: 33.7819, ACC: 0.28743961453437805\n",
      "Epoch: 110, Loss: 31.6358, ACC: 0.2753623127937317\n",
      "Epoch: 120, Loss: 29.6267, ACC: 0.3550724685192108\n",
      "Epoch: 130, Loss: 27.7454, ACC: 0.4855072498321533\n",
      "Epoch: 140, Loss: 25.9764, ACC: 0.5483092069625854\n",
      "Epoch: 150, Loss: 24.3140, ACC: 0.5410627722740173\n",
      "Epoch: 160, Loss: 22.7611, ACC: 0.483091801404953\n",
      "Epoch: 170, Loss: 21.3161, ACC: 0.4227053225040436\n",
      "Epoch: 180, Loss: 19.9736, ACC: 0.4468598961830139\n",
      "Epoch: 190, Loss: 18.7182, ACC: 0.43719807267189026\n",
      "Epoch: 200, Loss: 17.5437, ACC: 0.5048308968544006\n",
      "Epoch: 210, Loss: 16.4502, ACC: 0.5531401038169861\n",
      "Epoch: 220, Loss: 15.4310, ACC: 0.5869565010070801\n",
      "Epoch: 230, Loss: 14.4820, ACC: 0.6376811861991882\n",
      "Epoch: 240, Loss: 13.6031, ACC: 0.6545893549919128\n",
      "Epoch: 250, Loss: 12.7915, ACC: 0.7028985619544983\n",
      "Epoch: 260, Loss: 12.0427, ACC: 0.7222222089767456\n",
      "Epoch: 270, Loss: 11.3521, ACC: 0.7922705411911011\n",
      "Epoch: 280, Loss: 10.7152, ACC: 0.8188405632972717\n",
      "Epoch: 290, Loss: 10.1278, ACC: 0.8285024166107178\n",
      "Epoch: 300, Loss: 9.5861, ACC: 0.8478260636329651\n",
      "Epoch: 310, Loss: 9.0864, ACC: 0.804347813129425\n",
      "Epoch: 320, Loss: 8.6255, ACC: 0.760869562625885\n",
      "Epoch: 330, Loss: 8.2004, ACC: 0.7342995405197144\n",
      "Epoch: 340, Loss: 7.8080, ACC: 0.748792290687561\n",
      "Epoch: 350, Loss: 7.4459, ACC: 0.7584540843963623\n",
      "Epoch: 360, Loss: 7.1115, ACC: 0.772946834564209\n",
      "Epoch: 370, Loss: 6.8026, ACC: 0.8236715197563171\n",
      "Epoch: 380, Loss: 6.5171, ACC: 0.8550724387168884\n",
      "Epoch: 390, Loss: 6.2530, ACC: 0.8719806671142578\n",
      "Epoch: 400, Loss: 6.0087, ACC: 0.8792270421981812\n",
      "Epoch: 410, Loss: 5.7823, ACC: 0.9106280207633972\n",
      "Epoch: 420, Loss: 5.5724, ACC: 0.9227052927017212\n",
      "Epoch: 430, Loss: 5.3775, ACC: 0.97826087474823\n",
      "Epoch: 440, Loss: 5.1962, ACC: 1.0628019571304321\n",
      "Epoch: 450, Loss: 5.0275, ACC: 1.1038647890090942\n",
      "Epoch: 460, Loss: 4.8703, ACC: 1.1280193328857422\n",
      "Epoch: 470, Loss: 4.7236, ACC: 1.1328502893447876\n",
      "Epoch: 480, Loss: 4.5865, ACC: 1.1135265827178955\n",
      "Epoch: 490, Loss: 4.4582, ACC: 1.1521738767623901\n",
      "Epoch: 500, Loss: 4.3377, ACC: 1.1497584581375122\n",
      "Epoch: 510, Loss: 4.2243, ACC: 1.1739130020141602\n",
      "Epoch: 520, Loss: 4.1175, ACC: 1.185990333557129\n",
      "Epoch: 530, Loss: 4.0168, ACC: 1.1787439584732056\n",
      "Epoch: 540, Loss: 3.9218, ACC: 1.2560386657714844\n",
      "Epoch: 550, Loss: 3.8318, ACC: 1.251207709312439\n",
      "Epoch: 560, Loss: 3.7465, ACC: 1.2560386657714844\n",
      "Epoch: 570, Loss: 3.6656, ACC: 1.2439613342285156\n",
      "Epoch: 580, Loss: 3.5887, ACC: 1.2632850408554077\n",
      "Epoch: 590, Loss: 3.5153, ACC: 1.270531415939331\n",
      "Epoch: 600, Loss: 3.4449, ACC: 1.3091787099838257\n",
      "Epoch: 610, Loss: 3.3773, ACC: 1.338164210319519\n",
      "Epoch: 620, Loss: 3.3120, ACC: 1.3695652484893799\n",
      "Epoch: 630, Loss: 3.2490, ACC: 1.4009661674499512\n",
      "Epoch: 640, Loss: 3.1875, ACC: 1.4178744554519653\n",
      "Epoch: 650, Loss: 3.1275, ACC: 1.4251208305358887\n",
      "Epoch: 660, Loss: 3.0693, ACC: 1.4178744554519653\n",
      "Epoch: 670, Loss: 3.0124, ACC: 1.4082125425338745\n",
      "Epoch: 680, Loss: 2.9567, ACC: 1.3937197923660278\n",
      "Epoch: 690, Loss: 2.9025, ACC: 1.3985507488250732\n",
      "Epoch: 700, Loss: 2.8500, ACC: 1.3985507488250732\n",
      "Epoch: 710, Loss: 2.7991, ACC: 1.3937197923660278\n",
      "Epoch: 720, Loss: 2.7498, ACC: 1.3937197923660278\n",
      "Epoch: 730, Loss: 2.7020, ACC: 1.4082125425338745\n",
      "Epoch: 740, Loss: 2.6555, ACC: 1.4154589176177979\n",
      "Epoch: 750, Loss: 2.6103, ACC: 1.4299516677856445\n",
      "Epoch: 760, Loss: 2.5664, ACC: 1.4492753744125366\n",
      "Epoch: 770, Loss: 2.5237, ACC: 1.4637681245803833\n",
      "Epoch: 780, Loss: 2.4820, ACC: 1.4685990810394287\n",
      "Epoch: 790, Loss: 2.4415, ACC: 1.4734299182891846\n",
      "Epoch: 800, Loss: 2.4019, ACC: 1.4879226684570312\n",
      "Epoch: 810, Loss: 2.3633, ACC: 1.497584581375122\n",
      "Epoch: 820, Loss: 2.3257, ACC: 1.497584581375122\n",
      "Epoch: 830, Loss: 2.2889, ACC: 1.4855072498321533\n",
      "Epoch: 840, Loss: 2.2530, ACC: 1.4879226684570312\n",
      "Epoch: 850, Loss: 2.2179, ACC: 1.5\n",
      "Epoch: 860, Loss: 2.1836, ACC: 1.5144927501678467\n",
      "Epoch: 870, Loss: 2.1501, ACC: 1.567632794380188\n",
      "Epoch: 880, Loss: 2.1173, ACC: 1.5917874574661255\n",
      "Epoch: 890, Loss: 2.0853, ACC: 1.6280193328857422\n",
      "Epoch: 900, Loss: 2.0539, ACC: 1.6666666269302368\n",
      "Epoch: 910, Loss: 2.0233, ACC: 1.683574914932251\n",
      "Epoch: 920, Loss: 1.9932, ACC: 1.727053165435791\n",
      "Epoch: 930, Loss: 1.9639, ACC: 1.7342995405197144\n",
      "Epoch: 940, Loss: 1.9351, ACC: 1.7222222089767456\n",
      "Epoch: 950, Loss: 1.9070, ACC: 1.729468584060669\n",
      "Epoch: 960, Loss: 1.8794, ACC: 1.7608696222305298\n",
      "Epoch: 970, Loss: 1.8524, ACC: 1.7560386657714844\n",
      "Epoch: 980, Loss: 1.8260, ACC: 1.7560386657714844\n",
      "Epoch: 990, Loss: 1.8001, ACC: 1.772946834564209\n",
      "Epoch: 1000, Loss: 1.7747, ACC: 1.7850241661071777\n",
      "Epoch: 1010, Loss: 1.7499, ACC: 1.7753623723983765\n",
      "Epoch: 1020, Loss: 1.7256, ACC: 1.7801932096481323\n",
      "Epoch: 1030, Loss: 1.7017, ACC: 1.792270541191101\n",
      "Epoch: 1040, Loss: 1.6784, ACC: 1.7995169162750244\n",
      "Epoch: 1050, Loss: 1.6555, ACC: 1.814009666442871\n",
      "Epoch: 1060, Loss: 1.6331, ACC: 1.838164210319519\n",
      "Epoch: 1070, Loss: 1.6111, ACC: 1.8695652484893799\n",
      "Epoch: 1080, Loss: 1.5896, ACC: 1.8768116235733032\n",
      "Epoch: 1090, Loss: 1.5686, ACC: 1.9154589176177979\n",
      "Epoch: 1100, Loss: 1.5479, ACC: 1.9251208305358887\n",
      "Epoch: 1110, Loss: 1.5276, ACC: 1.9710144996643066\n",
      "Epoch: 1120, Loss: 1.5078, ACC: 2.0241546630859375\n",
      "Epoch: 1130, Loss: 1.4883, ACC: 2.048309087753296\n",
      "Epoch: 1140, Loss: 1.4693, ACC: 2.0724637508392334\n",
      "Epoch: 1150, Loss: 1.4506, ACC: 2.0748791694641113\n",
      "Epoch: 1160, Loss: 1.4323, ACC: 2.1014492511749268\n",
      "Epoch: 1170, Loss: 1.4143, ACC: 2.135265588760376\n",
      "Epoch: 1180, Loss: 1.3967, ACC: 2.1497583389282227\n",
      "Epoch: 1190, Loss: 1.3794, ACC: 2.17391300201416\n",
      "Epoch: 1200, Loss: 1.3625, ACC: 2.2149758338928223\n",
      "Epoch: 1210, Loss: 1.3459, ACC: 2.229468584060669\n",
      "Epoch: 1220, Loss: 1.3296, ACC: 2.268115997314453\n",
      "Epoch: 1230, Loss: 1.3137, ACC: 2.306763172149658\n",
      "Epoch: 1240, Loss: 1.2980, ACC: 2.3091788291931152\n",
      "Epoch: 1250, Loss: 1.2827, ACC: 2.3454105854034424\n",
      "Epoch: 1260, Loss: 1.2676, ACC: 2.3502416610717773\n",
      "Epoch: 1270, Loss: 1.2528, ACC: 2.3743960857391357\n",
      "Epoch: 1280, Loss: 1.2384, ACC: 2.400966167449951\n",
      "Epoch: 1290, Loss: 1.2241, ACC: 2.4227054119110107\n",
      "Epoch: 1300, Loss: 1.2102, ACC: 2.446859836578369\n",
      "Epoch: 1310, Loss: 1.1966, ACC: 2.4637680053710938\n",
      "Epoch: 1320, Loss: 1.1831, ACC: 2.4879226684570312\n",
      "Epoch: 1330, Loss: 1.1700, ACC: 2.507246494293213\n",
      "Epoch: 1340, Loss: 1.1571, ACC: 2.5314009189605713\n",
      "Epoch: 1350, Loss: 1.1444, ACC: 2.548309087753296\n",
      "Epoch: 1360, Loss: 1.1320, ACC: 2.5700483322143555\n",
      "Epoch: 1370, Loss: 1.1199, ACC: 2.5628018379211426\n",
      "Epoch: 1380, Loss: 1.1079, ACC: 2.553140163421631\n",
      "Epoch: 1390, Loss: 1.0962, ACC: 2.5700483322143555\n",
      "Epoch: 1400, Loss: 1.0847, ACC: 2.6014492511749268\n",
      "Epoch: 1410, Loss: 1.0734, ACC: 2.594202995300293\n",
      "Epoch: 1420, Loss: 1.0623, ACC: 2.6038646697998047\n",
      "Epoch: 1430, Loss: 1.0514, ACC: 2.591787338256836\n",
      "Epoch: 1440, Loss: 1.0407, ACC: 2.591787338256836\n",
      "Epoch: 1450, Loss: 1.0302, ACC: 2.591787338256836\n",
      "Epoch: 1460, Loss: 1.0200, ACC: 2.591787338256836\n",
      "Epoch: 1470, Loss: 1.0099, ACC: 2.6038646697998047\n",
      "Epoch: 1480, Loss: 0.9999, ACC: 2.591787338256836\n",
      "Epoch: 1490, Loss: 0.9902, ACC: 2.5748791694641113\n",
      "Epoch: 1500, Loss: 0.9807, ACC: 2.5700483322143555\n",
      "Epoch: 1510, Loss: 0.9713, ACC: 2.5314009189605713\n",
      "Epoch: 1520, Loss: 0.9621, ACC: 2.5241546630859375\n",
      "Epoch: 1530, Loss: 0.9530, ACC: 2.5362319946289062\n",
      "Epoch: 1540, Loss: 0.9441, ACC: 2.5362319946289062\n",
      "Epoch: 1550, Loss: 0.9354, ACC: 2.5265700817108154\n",
      "Epoch: 1560, Loss: 0.9268, ACC: 2.5265700817108154\n",
      "Epoch: 1570, Loss: 0.9184, ACC: 2.548309087753296\n",
      "Epoch: 1580, Loss: 0.9101, ACC: 2.5579710006713867\n",
      "Epoch: 1590, Loss: 0.9020, ACC: 2.555555582046509\n",
      "Epoch: 1600, Loss: 0.8940, ACC: 2.538647413253784\n",
      "Epoch: 1610, Loss: 0.8862, ACC: 2.538647413253784\n",
      "Epoch: 1620, Loss: 0.8785, ACC: 2.5289855003356934\n",
      "Epoch: 1630, Loss: 0.8709, ACC: 2.5289855003356934\n",
      "Epoch: 1640, Loss: 0.8634, ACC: 2.5700483322143555\n",
      "Epoch: 1650, Loss: 0.8561, ACC: 2.589371919631958\n",
      "Epoch: 1660, Loss: 0.8489, ACC: 2.599033832550049\n",
      "Epoch: 1670, Loss: 0.8418, ACC: 2.596618413925171\n",
      "Epoch: 1680, Loss: 0.8349, ACC: 2.6207728385925293\n",
      "Epoch: 1690, Loss: 0.8281, ACC: 2.6497583389282227\n",
      "Epoch: 1700, Loss: 0.8213, ACC: 2.6642510890960693\n",
      "Epoch: 1710, Loss: 0.8147, ACC: 2.67391300201416\n",
      "Epoch: 1720, Loss: 0.8082, ACC: 2.67391300201416\n",
      "Epoch: 1730, Loss: 0.8018, ACC: 2.6980676651000977\n",
      "Epoch: 1740, Loss: 0.7955, ACC: 2.7004830837249756\n",
      "Epoch: 1750, Loss: 0.7893, ACC: 2.7149758338928223\n",
      "Epoch: 1760, Loss: 0.7832, ACC: 2.724637746810913\n",
      "Epoch: 1770, Loss: 0.7772, ACC: 2.736715078353882\n",
      "Epoch: 1780, Loss: 0.7713, ACC: 2.7512078285217285\n",
      "Epoch: 1790, Loss: 0.7655, ACC: 2.7850241661071777\n",
      "Epoch: 1800, Loss: 0.7598, ACC: 2.7874395847320557\n",
      "Epoch: 1810, Loss: 0.7542, ACC: 2.7946860790252686\n",
      "Epoch: 1820, Loss: 0.7486, ACC: 2.82608699798584\n",
      "Epoch: 1830, Loss: 0.7432, ACC: 2.82608699798584\n",
      "Epoch: 1840, Loss: 0.7378, ACC: 2.82608699798584\n",
      "Epoch: 1850, Loss: 0.7325, ACC: 2.8478260040283203\n",
      "Epoch: 1860, Loss: 0.7273, ACC: 2.855072498321533\n",
      "Epoch: 1870, Loss: 0.7222, ACC: 2.864734411239624\n",
      "Epoch: 1880, Loss: 0.7171, ACC: 2.864734411239624\n",
      "Epoch: 1890, Loss: 0.7121, ACC: 2.8768115043640137\n",
      "Epoch: 1900, Loss: 0.7072, ACC: 2.8768115043640137\n",
      "Epoch: 1910, Loss: 0.7024, ACC: 2.8864734172821045\n",
      "Epoch: 1920, Loss: 0.6976, ACC: 2.8864734172821045\n",
      "Epoch: 1930, Loss: 0.6929, ACC: 2.8864734172821045\n",
      "Epoch: 1940, Loss: 0.6883, ACC: 2.8864734172821045\n",
      "Epoch: 1950, Loss: 0.6837, ACC: 2.8864734172821045\n",
      "Epoch: 1960, Loss: 0.6792, ACC: 2.8864734172821045\n",
      "Epoch: 1970, Loss: 0.6748, ACC: 2.8888888359069824\n",
      "Epoch: 1980, Loss: 0.6704, ACC: 2.903381586074829\n",
      "Epoch: 1990, Loss: 0.6661, ACC: 2.903381586074829\n",
      "Epoch: 2000, Loss: 0.6618, ACC: 2.903381586074829\n",
      "Epoch: 2010, Loss: 0.6576, ACC: 2.900966167449951\n",
      "Epoch: 2020, Loss: 0.6534, ACC: 2.908212661743164\n",
      "Epoch: 2030, Loss: 0.6493, ACC: 2.908212661743164\n",
      "Epoch: 2040, Loss: 0.6453, ACC: 2.908212661743164\n",
      "Epoch: 2050, Loss: 0.6413, ACC: 2.91304349899292\n",
      "Epoch: 2060, Loss: 0.6374, ACC: 2.917874336242676\n",
      "Epoch: 2070, Loss: 0.6335, ACC: 2.917874336242676\n",
      "Epoch: 2080, Loss: 0.6297, ACC: 2.9202897548675537\n",
      "Epoch: 2090, Loss: 0.6259, ACC: 2.9251208305358887\n",
      "Epoch: 2100, Loss: 0.6221, ACC: 2.9251208305358887\n",
      "Epoch: 2110, Loss: 0.6185, ACC: 2.9251208305358887\n",
      "Epoch: 2120, Loss: 0.6148, ACC: 2.949275255203247\n",
      "Epoch: 2130, Loss: 0.6112, ACC: 2.95652174949646\n",
      "Epoch: 2140, Loss: 0.6076, ACC: 2.946859836578369\n",
      "Epoch: 2150, Loss: 0.6041, ACC: 2.958937168121338\n",
      "Epoch: 2160, Loss: 0.6007, ACC: 2.9734299182891846\n",
      "Epoch: 2170, Loss: 0.5972, ACC: 2.9734299182891846\n",
      "Epoch: 2180, Loss: 0.5938, ACC: 2.9758453369140625\n",
      "Epoch: 2190, Loss: 0.5905, ACC: 2.9758453369140625\n",
      "Epoch: 2200, Loss: 0.5872, ACC: 2.9758453369140625\n",
      "Epoch: 2210, Loss: 0.5839, ACC: 2.9758453369140625\n",
      "Epoch: 2220, Loss: 0.5807, ACC: 2.9685990810394287\n",
      "Epoch: 2230, Loss: 0.5775, ACC: 2.9758453369140625\n",
      "Epoch: 2240, Loss: 0.5743, ACC: 2.9758453369140625\n",
      "Epoch: 2250, Loss: 0.5712, ACC: 2.9685990810394287\n",
      "Epoch: 2260, Loss: 0.5681, ACC: 2.9855072498321533\n",
      "Epoch: 2270, Loss: 0.5650, ACC: 2.9855072498321533\n",
      "Epoch: 2280, Loss: 0.5620, ACC: 2.9855072498321533\n",
      "Epoch: 2290, Loss: 0.5590, ACC: 2.9734299182891846\n",
      "Epoch: 2300, Loss: 0.5561, ACC: 2.992753505706787\n",
      "Epoch: 2310, Loss: 0.5531, ACC: 2.992753505706787\n",
      "Epoch: 2320, Loss: 0.5502, ACC: 2.992753505706787\n",
      "Epoch: 2330, Loss: 0.5474, ACC: 3.004830837249756\n",
      "Epoch: 2340, Loss: 0.5445, ACC: 3.004830837249756\n",
      "Epoch: 2350, Loss: 0.5417, ACC: 3.004830837249756\n",
      "Epoch: 2360, Loss: 0.5389, ACC: 3.004830837249756\n",
      "Epoch: 2370, Loss: 0.5362, ACC: 3.0265700817108154\n",
      "Epoch: 2380, Loss: 0.5334, ACC: 3.04347825050354\n",
      "Epoch: 2390, Loss: 0.5308, ACC: 3.04347825050354\n",
      "Epoch: 2400, Loss: 0.5281, ACC: 3.04347825050354\n",
      "Epoch: 2410, Loss: 0.5254, ACC: 3.050724744796753\n",
      "Epoch: 2420, Loss: 0.5228, ACC: 3.050724744796753\n",
      "Epoch: 2430, Loss: 0.5202, ACC: 3.050724744796753\n",
      "Epoch: 2440, Loss: 0.5176, ACC: 3.050724744796753\n",
      "Epoch: 2450, Loss: 0.5151, ACC: 3.0676329135894775\n",
      "Epoch: 2460, Loss: 0.5126, ACC: 3.0676329135894775\n",
      "Epoch: 2470, Loss: 0.5101, ACC: 3.0676329135894775\n",
      "Epoch: 2480, Loss: 0.5076, ACC: 3.084541082382202\n",
      "Epoch: 2490, Loss: 0.5052, ACC: 3.084541082382202\n",
      "Epoch: 2500, Loss: 0.5027, ACC: 3.084541082382202\n",
      "Epoch: 2510, Loss: 0.5003, ACC: 3.089371919631958\n",
      "Epoch: 2520, Loss: 0.4979, ACC: 3.089371919631958\n",
      "Epoch: 2530, Loss: 0.4956, ACC: 3.1014492511749268\n",
      "Epoch: 2540, Loss: 0.4932, ACC: 3.1014492511749268\n",
      "Epoch: 2550, Loss: 0.4909, ACC: 3.1062800884246826\n",
      "Epoch: 2560, Loss: 0.4886, ACC: 3.1111111640930176\n",
      "Epoch: 2570, Loss: 0.4863, ACC: 3.1135265827178955\n",
      "Epoch: 2580, Loss: 0.4840, ACC: 3.1135265827178955\n",
      "Epoch: 2590, Loss: 0.4818, ACC: 3.142512083053589\n",
      "Epoch: 2600, Loss: 0.4796, ACC: 3.142512083053589\n",
      "Epoch: 2610, Loss: 0.4774, ACC: 3.1497583389282227\n",
      "Epoch: 2620, Loss: 0.4752, ACC: 3.1618356704711914\n",
      "Epoch: 2630, Loss: 0.4730, ACC: 3.1618356704711914\n",
      "Epoch: 2640, Loss: 0.4708, ACC: 3.1618356704711914\n",
      "Epoch: 2650, Loss: 0.4687, ACC: 3.1618356704711914\n",
      "Epoch: 2660, Loss: 0.4666, ACC: 3.1618356704711914\n",
      "Epoch: 2670, Loss: 0.4645, ACC: 3.1642510890960693\n",
      "Epoch: 2680, Loss: 0.4624, ACC: 3.17391300201416\n",
      "Epoch: 2690, Loss: 0.4603, ACC: 3.17391300201416\n",
      "Epoch: 2700, Loss: 0.4583, ACC: 3.17391300201416\n",
      "Epoch: 2710, Loss: 0.4562, ACC: 3.188405752182007\n",
      "Epoch: 2720, Loss: 0.4542, ACC: 3.1908211708068848\n",
      "Epoch: 2730, Loss: 0.4522, ACC: 3.2004830837249756\n",
      "Epoch: 2740, Loss: 0.4502, ACC: 3.2101449966430664\n",
      "Epoch: 2750, Loss: 0.4482, ACC: 3.2101449966430664\n",
      "Epoch: 2760, Loss: 0.4463, ACC: 3.2101449966430664\n",
      "Epoch: 2770, Loss: 0.4443, ACC: 3.2101449966430664\n",
      "Epoch: 2780, Loss: 0.4424, ACC: 3.2173912525177\n",
      "Epoch: 2790, Loss: 0.4405, ACC: 3.2391304969787598\n",
      "Epoch: 2800, Loss: 0.4386, ACC: 3.2391304969787598\n",
      "Epoch: 2810, Loss: 0.4367, ACC: 3.2391304969787598\n",
      "Epoch: 2820, Loss: 0.4348, ACC: 3.2391304969787598\n",
      "Epoch: 2830, Loss: 0.4329, ACC: 3.2391304969787598\n",
      "Epoch: 2840, Loss: 0.4311, ACC: 3.2463767528533936\n",
      "Epoch: 2850, Loss: 0.4293, ACC: 3.2560386657714844\n",
      "Epoch: 2860, Loss: 0.4274, ACC: 3.2560386657714844\n",
      "Epoch: 2870, Loss: 0.4256, ACC: 3.265700578689575\n",
      "Epoch: 2880, Loss: 0.4238, ACC: 3.270531415939331\n",
      "Epoch: 2890, Loss: 0.4220, ACC: 3.268115997314453\n",
      "Epoch: 2900, Loss: 0.4202, ACC: 3.277777671813965\n",
      "Epoch: 2910, Loss: 0.4185, ACC: 3.2922704219818115\n",
      "Epoch: 2920, Loss: 0.4167, ACC: 3.2922704219818115\n",
      "Epoch: 2930, Loss: 0.4150, ACC: 3.2995169162750244\n",
      "Epoch: 2940, Loss: 0.4133, ACC: 3.2995169162750244\n",
      "Epoch: 2950, Loss: 0.4115, ACC: 3.3043477535247803\n",
      "Epoch: 2960, Loss: 0.4098, ACC: 3.3043477535247803\n",
      "Epoch: 2970, Loss: 0.4081, ACC: 3.3309178352355957\n",
      "Epoch: 2980, Loss: 0.4065, ACC: 3.3309178352355957\n",
      "Epoch: 2990, Loss: 0.4048, ACC: 3.321255922317505\n",
      "Epoch: 3000, Loss: 0.4031, ACC: 3.32608699798584\n",
      "Epoch: 3010, Loss: 0.4015, ACC: 3.32608699798584\n",
      "Epoch: 3020, Loss: 0.3998, ACC: 3.3357489109039307\n",
      "Epoch: 3030, Loss: 0.3982, ACC: 3.357487916946411\n",
      "Epoch: 3040, Loss: 0.3966, ACC: 3.357487916946411\n",
      "Epoch: 3050, Loss: 0.3950, ACC: 3.357487916946411\n",
      "Epoch: 3060, Loss: 0.3934, ACC: 3.357487916946411\n",
      "Epoch: 3070, Loss: 0.3918, ACC: 3.3526570796966553\n",
      "Epoch: 3080, Loss: 0.3902, ACC: 3.3526570796966553\n",
      "Epoch: 3090, Loss: 0.3886, ACC: 3.3526570796966553\n",
      "Epoch: 3100, Loss: 0.3870, ACC: 3.3478260040283203\n",
      "Epoch: 3110, Loss: 0.3855, ACC: 3.3405797481536865\n",
      "Epoch: 3120, Loss: 0.3839, ACC: 3.3405797481536865\n",
      "Epoch: 3130, Loss: 0.3824, ACC: 3.3405797481536865\n",
      "Epoch: 3140, Loss: 0.3808, ACC: 3.3405797481536865\n",
      "Epoch: 3150, Loss: 0.3793, ACC: 3.3454105854034424\n",
      "Epoch: 3160, Loss: 0.3778, ACC: 3.359903335571289\n",
      "Epoch: 3170, Loss: 0.3763, ACC: 3.359903335571289\n",
      "Epoch: 3180, Loss: 0.3748, ACC: 3.362318754196167\n",
      "Epoch: 3190, Loss: 0.3733, ACC: 3.367149829864502\n",
      "Epoch: 3200, Loss: 0.3718, ACC: 3.364734411239624\n",
      "Epoch: 3210, Loss: 0.3704, ACC: 3.36956524848938\n",
      "Epoch: 3220, Loss: 0.3689, ACC: 3.3743960857391357\n",
      "Epoch: 3230, Loss: 0.3674, ACC: 3.3768115043640137\n",
      "Epoch: 3240, Loss: 0.3660, ACC: 3.3768115043640137\n",
      "Epoch: 3250, Loss: 0.3645, ACC: 3.3888888359069824\n",
      "Epoch: 3260, Loss: 0.3631, ACC: 3.3888888359069824\n",
      "Epoch: 3270, Loss: 0.3617, ACC: 3.3961353302001953\n",
      "Epoch: 3280, Loss: 0.3603, ACC: 3.417874336242676\n",
      "Epoch: 3290, Loss: 0.3588, ACC: 3.4275362491607666\n",
      "Epoch: 3300, Loss: 0.3574, ACC: 3.461352586746216\n",
      "Epoch: 3310, Loss: 0.3560, ACC: 3.4685990810394287\n",
      "Epoch: 3320, Loss: 0.3547, ACC: 3.466183662414551\n",
      "Epoch: 3330, Loss: 0.3533, ACC: 3.466183662414551\n",
      "Epoch: 3340, Loss: 0.3519, ACC: 3.466183662414551\n",
      "Epoch: 3350, Loss: 0.3505, ACC: 3.466183662414551\n",
      "Epoch: 3360, Loss: 0.3492, ACC: 3.4710144996643066\n",
      "Epoch: 3370, Loss: 0.3478, ACC: 3.4710144996643066\n",
      "Epoch: 3380, Loss: 0.3464, ACC: 3.4710144996643066\n",
      "Epoch: 3390, Loss: 0.3451, ACC: 3.4710144996643066\n",
      "Epoch: 3400, Loss: 0.3438, ACC: 3.4758453369140625\n",
      "Epoch: 3410, Loss: 0.3424, ACC: 3.4782607555389404\n",
      "Epoch: 3420, Loss: 0.3411, ACC: 3.495169162750244\n",
      "Epoch: 3430, Loss: 0.3398, ACC: 3.497584581375122\n",
      "Epoch: 3440, Loss: 0.3385, ACC: 3.5120773315429688\n",
      "Epoch: 3450, Loss: 0.3372, ACC: 3.5217392444610596\n",
      "Epoch: 3460, Loss: 0.3359, ACC: 3.5362319946289062\n",
      "Epoch: 3470, Loss: 0.3346, ACC: 3.541062831878662\n",
      "Epoch: 3480, Loss: 0.3333, ACC: 3.54347825050354\n",
      "Epoch: 3490, Loss: 0.3320, ACC: 3.54347825050354\n",
      "Epoch: 3500, Loss: 0.3307, ACC: 3.54347825050354\n",
      "Epoch: 3510, Loss: 0.3294, ACC: 3.54347825050354\n",
      "Epoch: 3520, Loss: 0.3282, ACC: 3.54347825050354\n",
      "Epoch: 3530, Loss: 0.3269, ACC: 3.538647413253784\n",
      "Epoch: 3540, Loss: 0.3256, ACC: 3.545893669128418\n",
      "Epoch: 3550, Loss: 0.3244, ACC: 3.545893669128418\n",
      "Epoch: 3560, Loss: 0.3231, ACC: 3.545893669128418\n",
      "Epoch: 3570, Loss: 0.3219, ACC: 3.545893669128418\n",
      "Epoch: 3580, Loss: 0.3206, ACC: 3.545893669128418\n",
      "Epoch: 3590, Loss: 0.3194, ACC: 3.553140163421631\n",
      "Epoch: 3600, Loss: 0.3182, ACC: 3.5579710006713867\n",
      "Epoch: 3610, Loss: 0.3169, ACC: 3.5628018379211426\n",
      "Epoch: 3620, Loss: 0.3157, ACC: 3.5797102451324463\n",
      "Epoch: 3630, Loss: 0.3145, ACC: 3.5797102451324463\n",
      "Epoch: 3640, Loss: 0.3133, ACC: 3.6014492511749268\n",
      "Epoch: 3650, Loss: 0.3121, ACC: 3.591787338256836\n",
      "Epoch: 3660, Loss: 0.3109, ACC: 3.596618413925171\n",
      "Epoch: 3670, Loss: 0.3097, ACC: 3.596618413925171\n",
      "Epoch: 3680, Loss: 0.3085, ACC: 3.6135265827178955\n",
      "Epoch: 3690, Loss: 0.3073, ACC: 3.6135265827178955\n",
      "Epoch: 3700, Loss: 0.3061, ACC: 3.6135265827178955\n",
      "Epoch: 3710, Loss: 0.3049, ACC: 3.6207728385925293\n",
      "Epoch: 3720, Loss: 0.3037, ACC: 3.6207728385925293\n",
      "Epoch: 3730, Loss: 0.3026, ACC: 3.628019332885742\n",
      "Epoch: 3740, Loss: 0.3014, ACC: 3.6666667461395264\n",
      "Epoch: 3750, Loss: 0.3002, ACC: 3.678744077682495\n",
      "Epoch: 3760, Loss: 0.2991, ACC: 3.7173912525177\n",
      "Epoch: 3770, Loss: 0.2979, ACC: 3.729468584060669\n",
      "Epoch: 3780, Loss: 0.2968, ACC: 3.7463767528533936\n",
      "Epoch: 3790, Loss: 0.2956, ACC: 3.7463767528533936\n",
      "Epoch: 3800, Loss: 0.2945, ACC: 3.7608695030212402\n",
      "Epoch: 3810, Loss: 0.2933, ACC: 3.7608695030212402\n",
      "Epoch: 3820, Loss: 0.2922, ACC: 3.777777671813965\n",
      "Epoch: 3830, Loss: 0.2910, ACC: 3.7995169162750244\n",
      "Epoch: 3840, Loss: 0.2899, ACC: 3.7995169162750244\n",
      "Epoch: 3850, Loss: 0.2888, ACC: 3.821255922317505\n",
      "Epoch: 3860, Loss: 0.2876, ACC: 3.8381643295288086\n",
      "Epoch: 3870, Loss: 0.2865, ACC: 3.8309178352355957\n",
      "Epoch: 3880, Loss: 0.2854, ACC: 3.8309178352355957\n",
      "Epoch: 3890, Loss: 0.2843, ACC: 3.8309178352355957\n",
      "Epoch: 3900, Loss: 0.2832, ACC: 3.8357489109039307\n",
      "Epoch: 3910, Loss: 0.2821, ACC: 3.8357489109039307\n",
      "Epoch: 3920, Loss: 0.2810, ACC: 3.8357489109039307\n",
      "Epoch: 3930, Loss: 0.2798, ACC: 3.8357489109039307\n",
      "Epoch: 3940, Loss: 0.2787, ACC: 3.8357489109039307\n",
      "Epoch: 3950, Loss: 0.2777, ACC: 3.8285024166107178\n",
      "Epoch: 3960, Loss: 0.2766, ACC: 3.8405797481536865\n",
      "Epoch: 3970, Loss: 0.2755, ACC: 3.8405797481536865\n",
      "Epoch: 3980, Loss: 0.2744, ACC: 3.862318754196167\n",
      "Epoch: 3990, Loss: 0.2733, ACC: 3.871980667114258\n",
      "Epoch: 4000, Loss: 0.2722, ACC: 3.8768115043640137\n",
      "Epoch: 4010, Loss: 0.2712, ACC: 3.8864734172821045\n",
      "Epoch: 4020, Loss: 0.2701, ACC: 3.8864734172821045\n",
      "Epoch: 4030, Loss: 0.2690, ACC: 3.8864734172821045\n",
      "Epoch: 4040, Loss: 0.2680, ACC: 3.8961353302001953\n",
      "Epoch: 4050, Loss: 0.2669, ACC: 3.915458917617798\n",
      "Epoch: 4060, Loss: 0.2658, ACC: 3.9202897548675537\n",
      "Epoch: 4070, Loss: 0.2648, ACC: 3.9202897548675537\n",
      "Epoch: 4080, Loss: 0.2637, ACC: 3.9299516677856445\n",
      "Epoch: 4090, Loss: 0.2627, ACC: 3.9299516677856445\n",
      "Epoch: 4100, Loss: 0.2616, ACC: 3.949275255203247\n",
      "Epoch: 4110, Loss: 0.2606, ACC: 3.9855072498321533\n",
      "Epoch: 4120, Loss: 0.2596, ACC: 4.009661674499512\n",
      "Epoch: 4130, Loss: 0.2585, ACC: 4.009661674499512\n",
      "Epoch: 4140, Loss: 0.2575, ACC: 4.009661674499512\n",
      "Epoch: 4150, Loss: 0.2565, ACC: 4.0241546630859375\n",
      "Epoch: 4160, Loss: 0.2554, ACC: 4.041062831878662\n",
      "Epoch: 4170, Loss: 0.2544, ACC: 4.041062831878662\n",
      "Epoch: 4180, Loss: 0.2534, ACC: 4.053140163421631\n",
      "Epoch: 4190, Loss: 0.2524, ACC: 4.057971000671387\n",
      "Epoch: 4200, Loss: 0.2514, ACC: 4.082125663757324\n",
      "Epoch: 4210, Loss: 0.2503, ACC: 4.101449489593506\n",
      "Epoch: 4220, Loss: 0.2493, ACC: 4.128019332885742\n",
      "Epoch: 4230, Loss: 0.2483, ACC: 4.140096664428711\n",
      "Epoch: 4240, Loss: 0.2473, ACC: 4.1545891761779785\n",
      "Epoch: 4250, Loss: 0.2463, ACC: 4.166666507720947\n",
      "Epoch: 4260, Loss: 0.2453, ACC: 4.183574676513672\n",
      "Epoch: 4270, Loss: 0.2443, ACC: 4.198067665100098\n",
      "Epoch: 4280, Loss: 0.2433, ACC: 4.212560176849365\n",
      "Epoch: 4290, Loss: 0.2424, ACC: 4.222222328186035\n",
      "Epoch: 4300, Loss: 0.2414, ACC: 4.222222328186035\n",
      "Epoch: 4310, Loss: 0.2404, ACC: 4.229468822479248\n",
      "Epoch: 4320, Loss: 0.2394, ACC: 4.229468822479248\n",
      "Epoch: 4330, Loss: 0.2385, ACC: 4.229468822479248\n",
      "Epoch: 4340, Loss: 0.2375, ACC: 4.253623008728027\n",
      "Epoch: 4350, Loss: 0.2365, ACC: 4.26086950302124\n",
      "Epoch: 4360, Loss: 0.2356, ACC: 4.268115997314453\n",
      "Epoch: 4370, Loss: 0.2346, ACC: 4.268115997314453\n",
      "Epoch: 4380, Loss: 0.2337, ACC: 4.268115997314453\n",
      "Epoch: 4390, Loss: 0.2327, ACC: 4.268115997314453\n",
      "Epoch: 4400, Loss: 0.2318, ACC: 4.268115997314453\n",
      "Epoch: 4410, Loss: 0.2308, ACC: 4.272946834564209\n",
      "Epoch: 4420, Loss: 0.2299, ACC: 4.268115997314453\n",
      "Epoch: 4430, Loss: 0.2290, ACC: 4.277777671813965\n",
      "Epoch: 4440, Loss: 0.2280, ACC: 4.277777671813965\n",
      "Epoch: 4450, Loss: 0.2271, ACC: 4.285024166107178\n",
      "Epoch: 4460, Loss: 0.2262, ACC: 4.299516677856445\n",
      "Epoch: 4470, Loss: 0.2252, ACC: 4.299516677856445\n",
      "Epoch: 4480, Loss: 0.2243, ACC: 4.314009666442871\n",
      "Epoch: 4490, Loss: 0.2234, ACC: 4.314009666442871\n",
      "Epoch: 4500, Loss: 0.2225, ACC: 4.32608699798584\n",
      "Epoch: 4510, Loss: 0.2216, ACC: 4.328502178192139\n",
      "Epoch: 4520, Loss: 0.2207, ACC: 4.330917835235596\n",
      "Epoch: 4530, Loss: 0.2198, ACC: 4.330917835235596\n",
      "Epoch: 4540, Loss: 0.2189, ACC: 4.340579509735107\n",
      "Epoch: 4550, Loss: 0.2180, ACC: 4.340579509735107\n",
      "Epoch: 4560, Loss: 0.2171, ACC: 4.340579509735107\n",
      "Epoch: 4570, Loss: 0.2162, ACC: 4.340579509735107\n",
      "Epoch: 4580, Loss: 0.2153, ACC: 4.350241661071777\n",
      "Epoch: 4590, Loss: 0.2145, ACC: 4.350241661071777\n",
      "Epoch: 4600, Loss: 0.2136, ACC: 4.350241661071777\n",
      "Epoch: 4610, Loss: 0.2127, ACC: 4.350241661071777\n",
      "Epoch: 4620, Loss: 0.2118, ACC: 4.367149829864502\n",
      "Epoch: 4630, Loss: 0.2110, ACC: 4.405797004699707\n",
      "Epoch: 4640, Loss: 0.2101, ACC: 4.420289993286133\n",
      "Epoch: 4650, Loss: 0.2093, ACC: 4.4347825050354\n",
      "Epoch: 4660, Loss: 0.2084, ACC: 4.439613342285156\n",
      "Epoch: 4670, Loss: 0.2076, ACC: 4.439613342285156\n",
      "Epoch: 4680, Loss: 0.2067, ACC: 4.442028999328613\n",
      "Epoch: 4690, Loss: 0.2059, ACC: 4.442028999328613\n",
      "Epoch: 4700, Loss: 0.2050, ACC: 4.454106330871582\n",
      "Epoch: 4710, Loss: 0.2042, ACC: 4.454106330871582\n",
      "Epoch: 4720, Loss: 0.2034, ACC: 4.454106330871582\n",
      "Epoch: 4730, Loss: 0.2026, ACC: 4.454106330871582\n",
      "Epoch: 4740, Loss: 0.2017, ACC: 4.454106330871582\n",
      "Epoch: 4750, Loss: 0.2009, ACC: 4.454106330871582\n",
      "Epoch: 4760, Loss: 0.2001, ACC: 4.456521511077881\n",
      "Epoch: 4770, Loss: 0.1993, ACC: 4.456521511077881\n",
      "Epoch: 4780, Loss: 0.1985, ACC: 4.458937168121338\n",
      "Epoch: 4790, Loss: 0.1977, ACC: 4.463768005371094\n",
      "Epoch: 4800, Loss: 0.1969, ACC: 4.473430156707764\n",
      "Epoch: 4810, Loss: 0.1961, ACC: 4.473430156707764\n",
      "Epoch: 4820, Loss: 0.1953, ACC: 4.473430156707764\n",
      "Epoch: 4830, Loss: 0.1946, ACC: 4.490338325500488\n",
      "Epoch: 4840, Loss: 0.1938, ACC: 4.490338325500488\n",
      "Epoch: 4850, Loss: 0.1930, ACC: 4.490338325500488\n",
      "Epoch: 4860, Loss: 0.1922, ACC: 4.502415657043457\n",
      "Epoch: 4870, Loss: 0.1915, ACC: 4.502415657043457\n",
      "Epoch: 4880, Loss: 0.1907, ACC: 4.514492988586426\n",
      "Epoch: 4890, Loss: 0.1900, ACC: 4.514492988586426\n",
      "Epoch: 4900, Loss: 0.1892, ACC: 4.514492988586426\n",
      "Epoch: 4910, Loss: 0.1885, ACC: 4.514492988586426\n",
      "Epoch: 4920, Loss: 0.1877, ACC: 4.514492988586426\n",
      "Epoch: 4930, Loss: 0.1870, ACC: 4.514492988586426\n",
      "Epoch: 4940, Loss: 0.1863, ACC: 4.514492988586426\n",
      "Epoch: 4950, Loss: 0.1855, ACC: 4.5217390060424805\n",
      "Epoch: 4960, Loss: 0.1848, ACC: 4.526569843292236\n",
      "Epoch: 4970, Loss: 0.1841, ACC: 4.541062831878662\n",
      "Epoch: 4980, Loss: 0.1834, ACC: 4.541062831878662\n",
      "Epoch: 4990, Loss: 0.1827, ACC: 4.541062831878662\n",
      "Epoch: 5000, Loss: 0.1820, ACC: 4.541062831878662\n",
      "Epoch: 5010, Loss: 0.1813, ACC: 4.545893669128418\n",
      "Epoch: 5020, Loss: 0.1806, ACC: 4.550724506378174\n",
      "Epoch: 5030, Loss: 0.1799, ACC: 4.550724506378174\n",
      "Epoch: 5040, Loss: 0.1792, ACC: 4.550724506378174\n",
      "Epoch: 5050, Loss: 0.1785, ACC: 4.553140163421631\n",
      "Epoch: 5060, Loss: 0.1779, ACC: 4.553140163421631\n",
      "Epoch: 5070, Loss: 0.1772, ACC: 4.560386657714844\n",
      "Epoch: 5080, Loss: 0.1765, ACC: 4.560386657714844\n",
      "Epoch: 5090, Loss: 0.1759, ACC: 4.579710006713867\n",
      "Epoch: 5100, Loss: 0.1752, ACC: 4.606280326843262\n",
      "Epoch: 5110, Loss: 0.1746, ACC: 4.611111164093018\n",
      "Epoch: 5120, Loss: 0.1739, ACC: 4.611111164093018\n",
      "Epoch: 5130, Loss: 0.1733, ACC: 4.611111164093018\n",
      "Epoch: 5140, Loss: 0.1727, ACC: 4.6183576583862305\n",
      "Epoch: 5150, Loss: 0.1720, ACC: 4.6183576583862305\n",
      "Epoch: 5160, Loss: 0.1714, ACC: 4.6086955070495605\n",
      "Epoch: 5170, Loss: 0.1708, ACC: 4.6086955070495605\n",
      "Epoch: 5180, Loss: 0.1702, ACC: 4.613526344299316\n",
      "Epoch: 5190, Loss: 0.1696, ACC: 4.613526344299316\n",
      "Epoch: 5200, Loss: 0.1689, ACC: 4.613526344299316\n",
      "Epoch: 5210, Loss: 0.1683, ACC: 4.628019332885742\n",
      "Epoch: 5220, Loss: 0.1678, ACC: 4.632850170135498\n",
      "Epoch: 5230, Loss: 0.1672, ACC: 4.649758338928223\n",
      "Epoch: 5240, Loss: 0.1666, ACC: 4.6570048332214355\n",
      "Epoch: 5250, Loss: 0.1660, ACC: 4.6570048332214355\n",
      "Epoch: 5260, Loss: 0.1654, ACC: 4.695652008056641\n",
      "Epoch: 5270, Loss: 0.1649, ACC: 4.710144996643066\n",
      "Epoch: 5280, Loss: 0.1643, ACC: 4.719806671142578\n",
      "Epoch: 5290, Loss: 0.1637, ACC: 4.727053165435791\n",
      "Epoch: 5300, Loss: 0.1632, ACC: 4.727053165435791\n",
      "Epoch: 5310, Loss: 0.1626, ACC: 4.727053165435791\n",
      "Epoch: 5320, Loss: 0.1621, ACC: 4.727053165435791\n",
      "Epoch: 5330, Loss: 0.1615, ACC: 4.729468822479248\n",
      "Epoch: 5340, Loss: 0.1610, ACC: 4.756038665771484\n",
      "Epoch: 5350, Loss: 0.1605, ACC: 4.756038665771484\n",
      "Epoch: 5360, Loss: 0.1599, ACC: 4.765700340270996\n",
      "Epoch: 5370, Loss: 0.1594, ACC: 4.765700340270996\n",
      "Epoch: 5380, Loss: 0.1589, ACC: 4.765700340270996\n",
      "Epoch: 5390, Loss: 0.1584, ACC: 4.765700340270996\n",
      "Epoch: 5400, Loss: 0.1579, ACC: 4.765700340270996\n",
      "Epoch: 5410, Loss: 0.1574, ACC: 4.765700340270996\n",
      "Epoch: 5420, Loss: 0.1569, ACC: 4.777777671813965\n",
      "Epoch: 5430, Loss: 0.1564, ACC: 4.777777671813965\n",
      "Epoch: 5440, Loss: 0.1559, ACC: 4.782608509063721\n",
      "Epoch: 5450, Loss: 0.1554, ACC: 4.789855003356934\n",
      "Epoch: 5460, Loss: 0.1549, ACC: 4.789855003356934\n",
      "Epoch: 5470, Loss: 0.1545, ACC: 4.782608509063721\n",
      "Epoch: 5480, Loss: 0.1540, ACC: 4.785024166107178\n",
      "Epoch: 5490, Loss: 0.1535, ACC: 4.7971014976501465\n",
      "Epoch: 5500, Loss: 0.1531, ACC: 4.801932334899902\n",
      "Epoch: 5510, Loss: 0.1526, ACC: 4.792270660400391\n",
      "Epoch: 5520, Loss: 0.1521, ACC: 4.806763172149658\n",
      "Epoch: 5530, Loss: 0.1517, ACC: 4.821256160736084\n",
      "Epoch: 5540, Loss: 0.1513, ACC: 4.830917835235596\n",
      "Epoch: 5550, Loss: 0.1508, ACC: 4.821256160736084\n",
      "Epoch: 5560, Loss: 0.1504, ACC: 4.82608699798584\n",
      "Epoch: 5570, Loss: 0.1499, ACC: 4.8454108238220215\n",
      "Epoch: 5580, Loss: 0.1495, ACC: 4.852656841278076\n",
      "Epoch: 5590, Loss: 0.1491, ACC: 4.862318992614746\n",
      "Epoch: 5600, Loss: 0.1487, ACC: 4.867149829864502\n",
      "Epoch: 5610, Loss: 0.1483, ACC: 4.869565010070801\n",
      "Epoch: 5620, Loss: 0.1479, ACC: 4.886473655700684\n",
      "Epoch: 5630, Loss: 0.1475, ACC: 4.888888835906982\n",
      "Epoch: 5640, Loss: 0.1471, ACC: 4.896135330200195\n",
      "Epoch: 5650, Loss: 0.1467, ACC: 4.898550510406494\n",
      "Epoch: 5660, Loss: 0.1463, ACC: 4.900966167449951\n",
      "Epoch: 5670, Loss: 0.1459, ACC: 4.908212661743164\n",
      "Epoch: 5680, Loss: 0.1455, ACC: 4.9299516677856445\n",
      "Epoch: 5690, Loss: 0.1451, ACC: 4.937198162078857\n",
      "Epoch: 5700, Loss: 0.1448, ACC: 4.939613342285156\n",
      "Epoch: 5710, Loss: 0.1444, ACC: 4.954106330871582\n",
      "Epoch: 5720, Loss: 0.1440, ACC: 4.954106330871582\n",
      "Epoch: 5730, Loss: 0.1437, ACC: 4.951690673828125\n",
      "Epoch: 5740, Loss: 0.1433, ACC: 4.956521511077881\n",
      "Epoch: 5750, Loss: 0.1429, ACC: 4.954106330871582\n",
      "Epoch: 5760, Loss: 0.1426, ACC: 4.94444465637207\n",
      "Epoch: 5770, Loss: 0.1422, ACC: 4.94444465637207\n",
      "Epoch: 5780, Loss: 0.1419, ACC: 4.94444465637207\n",
      "Epoch: 5790, Loss: 0.1416, ACC: 4.946859836578369\n",
      "Epoch: 5800, Loss: 0.1412, ACC: 4.954106330871582\n",
      "Epoch: 5810, Loss: 0.1409, ACC: 4.954106330871582\n",
      "Epoch: 5820, Loss: 0.1406, ACC: 4.954106330871582\n",
      "Epoch: 5830, Loss: 0.1402, ACC: 4.954106330871582\n",
      "Epoch: 5840, Loss: 0.1399, ACC: 4.958937168121338\n",
      "Epoch: 5850, Loss: 0.1396, ACC: 4.954106330871582\n",
      "Epoch: 5860, Loss: 0.1393, ACC: 4.954106330871582\n",
      "Epoch: 5870, Loss: 0.1390, ACC: 4.961352825164795\n",
      "Epoch: 5880, Loss: 0.1387, ACC: 4.961352825164795\n",
      "Epoch: 5890, Loss: 0.1384, ACC: 4.961352825164795\n",
      "Epoch: 5900, Loss: 0.1381, ACC: 4.961352825164795\n",
      "Epoch: 5910, Loss: 0.1378, ACC: 4.961352825164795\n",
      "Epoch: 5920, Loss: 0.1375, ACC: 4.961352825164795\n",
      "Epoch: 5930, Loss: 0.1372, ACC: 4.954106330871582\n",
      "Epoch: 5940, Loss: 0.1369, ACC: 4.954106330871582\n",
      "Epoch: 5950, Loss: 0.1366, ACC: 4.954106330871582\n",
      "Epoch: 5960, Loss: 0.1363, ACC: 4.954106330871582\n",
      "Epoch: 5970, Loss: 0.1361, ACC: 4.954106330871582\n",
      "Epoch: 5980, Loss: 0.1358, ACC: 4.954106330871582\n",
      "Epoch: 5990, Loss: 0.1355, ACC: 4.954106330871582\n",
      "Epoch: 6000, Loss: 0.1353, ACC: 4.954106330871582\n",
      "Epoch: 6010, Loss: 0.1350, ACC: 4.966183662414551\n",
      "Epoch: 6020, Loss: 0.1347, ACC: 4.980676174163818\n",
      "Epoch: 6030, Loss: 0.1345, ACC: 4.987922668457031\n",
      "Epoch: 6040, Loss: 0.1342, ACC: 4.987922668457031\n",
      "Epoch: 6050, Loss: 0.1340, ACC: 4.987922668457031\n",
      "Epoch: 6060, Loss: 0.1337, ACC: 4.987922668457031\n",
      "Epoch: 6070, Loss: 0.1335, ACC: 4.987922668457031\n",
      "Epoch: 6080, Loss: 0.1332, ACC: 4.987922668457031\n",
      "Epoch: 6090, Loss: 0.1330, ACC: 4.9782609939575195\n",
      "Epoch: 6100, Loss: 0.1328, ACC: 4.9782609939575195\n",
      "Epoch: 6110, Loss: 0.1325, ACC: 4.9782609939575195\n",
      "Epoch: 6120, Loss: 0.1323, ACC: 4.9782609939575195\n",
      "Epoch: 6130, Loss: 0.1321, ACC: 4.9782609939575195\n",
      "Epoch: 6140, Loss: 0.1318, ACC: 4.987922668457031\n",
      "Epoch: 6150, Loss: 0.1316, ACC: 4.987922668457031\n",
      "Epoch: 6160, Loss: 0.1314, ACC: 4.987922668457031\n",
      "Epoch: 6170, Loss: 0.1312, ACC: 4.987922668457031\n",
      "Epoch: 6180, Loss: 0.1310, ACC: 4.983091831207275\n",
      "Epoch: 6190, Loss: 0.1308, ACC: 4.985507011413574\n",
      "Epoch: 6200, Loss: 0.1306, ACC: 4.983091831207275\n",
      "Epoch: 6210, Loss: 0.1303, ACC: 4.983091831207275\n",
      "Epoch: 6220, Loss: 0.1301, ACC: 4.987922668457031\n",
      "Epoch: 6230, Loss: 0.1299, ACC: 4.987922668457031\n",
      "Epoch: 6240, Loss: 0.1297, ACC: 4.987922668457031\n",
      "Epoch: 6250, Loss: 0.1295, ACC: 4.987922668457031\n",
      "Epoch: 6260, Loss: 0.1293, ACC: 4.987922668457031\n",
      "Epoch: 6270, Loss: 0.1292, ACC: 4.987922668457031\n",
      "Epoch: 6280, Loss: 0.1290, ACC: 4.987922668457031\n",
      "Epoch: 6290, Loss: 0.1288, ACC: 4.987922668457031\n",
      "Epoch: 6300, Loss: 0.1286, ACC: 4.987922668457031\n",
      "Epoch: 6310, Loss: 0.1284, ACC: 4.987922668457031\n",
      "Epoch: 6320, Loss: 0.1282, ACC: 4.987922668457031\n",
      "Epoch: 6330, Loss: 0.1280, ACC: 4.990338325500488\n",
      "Epoch: 6340, Loss: 0.1279, ACC: 4.990338325500488\n",
      "Epoch: 6350, Loss: 0.1277, ACC: 5.004830837249756\n",
      "Epoch: 6360, Loss: 0.1275, ACC: 5.007246494293213\n",
      "Epoch: 6370, Loss: 0.1274, ACC: 5.007246494293213\n",
      "Epoch: 6380, Loss: 0.1272, ACC: 5.007246494293213\n",
      "Epoch: 6390, Loss: 0.1270, ACC: 5.007246494293213\n",
      "Epoch: 6400, Loss: 0.1269, ACC: 5.007246494293213\n",
      "Epoch: 6410, Loss: 0.1267, ACC: 5.007246494293213\n",
      "Epoch: 6420, Loss: 0.1265, ACC: 5.007246494293213\n",
      "Epoch: 6430, Loss: 0.1264, ACC: 5.007246494293213\n",
      "Epoch: 6440, Loss: 0.1262, ACC: 5.007246494293213\n",
      "Epoch: 6450, Loss: 0.1261, ACC: 5.009661674499512\n",
      "Epoch: 6460, Loss: 0.1259, ACC: 5.009661674499512\n",
      "Epoch: 6470, Loss: 0.1258, ACC: 5.009661674499512\n",
      "Epoch: 6480, Loss: 0.1256, ACC: 5.0241546630859375\n",
      "Epoch: 6490, Loss: 0.1255, ACC: 5.0241546630859375\n",
      "Epoch: 6500, Loss: 0.1253, ACC: 5.0241546630859375\n",
      "Epoch: 6510, Loss: 0.1252, ACC: 5.0241546630859375\n",
      "Epoch: 6520, Loss: 0.1250, ACC: 5.0241546630859375\n",
      "Epoch: 6530, Loss: 0.1249, ACC: 5.0241546630859375\n",
      "Epoch: 6540, Loss: 0.1248, ACC: 5.0217390060424805\n",
      "Epoch: 6550, Loss: 0.1246, ACC: 5.0217390060424805\n",
      "Epoch: 6560, Loss: 0.1245, ACC: 5.0217390060424805\n",
      "Epoch: 6570, Loss: 0.1244, ACC: 5.0217390060424805\n",
      "Epoch: 6580, Loss: 0.1242, ACC: 5.028985500335693\n",
      "Epoch: 6590, Loss: 0.1241, ACC: 5.028985500335693\n",
      "Epoch: 6600, Loss: 0.1240, ACC: 5.028985500335693\n",
      "Epoch: 6610, Loss: 0.1238, ACC: 5.028985500335693\n",
      "Epoch: 6620, Loss: 0.1237, ACC: 5.028985500335693\n",
      "Epoch: 6630, Loss: 0.1236, ACC: 5.028985500335693\n",
      "Epoch: 6640, Loss: 0.1235, ACC: 5.028985500335693\n",
      "Epoch: 6650, Loss: 0.1234, ACC: 5.026569843292236\n",
      "Epoch: 6660, Loss: 0.1232, ACC: 5.0241546630859375\n",
      "Epoch: 6670, Loss: 0.1231, ACC: 5.0241546630859375\n",
      "Epoch: 6680, Loss: 0.1230, ACC: 5.0241546630859375\n",
      "Epoch: 6690, Loss: 0.1229, ACC: 5.0\n",
      "Epoch: 6700, Loss: 0.1228, ACC: 5.0\n",
      "Epoch: 6710, Loss: 0.1227, ACC: 5.0\n",
      "Epoch: 6720, Loss: 0.1226, ACC: 5.0\n",
      "Epoch: 6730, Loss: 0.1224, ACC: 5.004830837249756\n",
      "Epoch: 6740, Loss: 0.1223, ACC: 5.004830837249756\n",
      "Epoch: 6750, Loss: 0.1222, ACC: 5.004830837249756\n",
      "Epoch: 6760, Loss: 0.1221, ACC: 5.004830837249756\n",
      "Epoch: 6770, Loss: 0.1220, ACC: 5.004830837249756\n",
      "Epoch: 6780, Loss: 0.1219, ACC: 5.004830837249756\n",
      "Epoch: 6790, Loss: 0.1218, ACC: 5.004830837249756\n",
      "Epoch: 6800, Loss: 0.1217, ACC: 5.014492988586426\n",
      "Epoch: 6810, Loss: 0.1216, ACC: 5.019323825836182\n",
      "Epoch: 6820, Loss: 0.1215, ACC: 5.019323825836182\n",
      "Epoch: 6830, Loss: 0.1214, ACC: 5.019323825836182\n",
      "Epoch: 6840, Loss: 0.1213, ACC: 5.014492988586426\n",
      "Epoch: 6850, Loss: 0.1213, ACC: 5.014492988586426\n",
      "Epoch: 6860, Loss: 0.1212, ACC: 5.014492988586426\n",
      "Epoch: 6870, Loss: 0.1211, ACC: 5.014492988586426\n",
      "Epoch: 6880, Loss: 0.1210, ACC: 5.014492988586426\n",
      "Epoch: 6890, Loss: 0.1209, ACC: 5.014492988586426\n",
      "Epoch: 6900, Loss: 0.1208, ACC: 5.012077331542969\n",
      "Epoch: 6910, Loss: 0.1207, ACC: 5.012077331542969\n",
      "Epoch: 6920, Loss: 0.1206, ACC: 5.012077331542969\n",
      "Epoch: 6930, Loss: 0.1206, ACC: 5.004830837249756\n",
      "Epoch: 6940, Loss: 0.1205, ACC: 5.004830837249756\n",
      "Epoch: 6950, Loss: 0.1204, ACC: 5.004830837249756\n",
      "Epoch: 6960, Loss: 0.1203, ACC: 5.004830837249756\n",
      "Epoch: 6970, Loss: 0.1202, ACC: 5.004830837249756\n",
      "Epoch: 6980, Loss: 0.1201, ACC: 5.007246494293213\n",
      "Epoch: 6990, Loss: 0.1201, ACC: 5.007246494293213\n",
      "Epoch: 7000, Loss: 0.1200, ACC: 5.009661674499512\n",
      "Epoch: 7010, Loss: 0.1199, ACC: 5.012077331542969\n",
      "Epoch: 7020, Loss: 0.1198, ACC: 5.012077331542969\n",
      "Epoch: 7030, Loss: 0.1198, ACC: 5.019323825836182\n",
      "Epoch: 7040, Loss: 0.1197, ACC: 5.014492988586426\n",
      "Epoch: 7050, Loss: 0.1196, ACC: 5.009661674499512\n",
      "Epoch: 7060, Loss: 0.1195, ACC: 5.019323825836182\n",
      "Epoch: 7070, Loss: 0.1195, ACC: 5.019323825836182\n",
      "Epoch: 7080, Loss: 0.1194, ACC: 5.019323825836182\n",
      "Epoch: 7090, Loss: 0.1193, ACC: 5.014492988586426\n",
      "Epoch: 7100, Loss: 0.1193, ACC: 5.014492988586426\n",
      "Epoch: 7110, Loss: 0.1192, ACC: 5.014492988586426\n",
      "Epoch: 7120, Loss: 0.1191, ACC: 5.014492988586426\n",
      "Epoch: 7130, Loss: 0.1191, ACC: 5.014492988586426\n",
      "Epoch: 7140, Loss: 0.1190, ACC: 5.014492988586426\n",
      "Epoch: 7150, Loss: 0.1190, ACC: 5.014492988586426\n",
      "Epoch: 7160, Loss: 0.1189, ACC: 5.014492988586426\n",
      "Epoch: 7170, Loss: 0.1188, ACC: 5.014492988586426\n",
      "Epoch: 7180, Loss: 0.1188, ACC: 5.014492988586426\n",
      "Epoch: 7190, Loss: 0.1187, ACC: 5.014492988586426\n",
      "Epoch: 7200, Loss: 0.1187, ACC: 5.014492988586426\n",
      "Epoch: 7210, Loss: 0.1186, ACC: 5.014492988586426\n",
      "Epoch: 7220, Loss: 0.1185, ACC: 5.014492988586426\n",
      "Epoch: 7230, Loss: 0.1185, ACC: 5.014492988586426\n",
      "Epoch: 7240, Loss: 0.1184, ACC: 5.014492988586426\n",
      "Epoch: 7250, Loss: 0.1184, ACC: 5.014492988586426\n",
      "Epoch: 7260, Loss: 0.1183, ACC: 5.014492988586426\n",
      "Epoch: 7270, Loss: 0.1183, ACC: 5.014492988586426\n",
      "Epoch: 7280, Loss: 0.1182, ACC: 5.014492988586426\n",
      "Epoch: 7290, Loss: 0.1182, ACC: 5.014492988586426\n",
      "Epoch: 7300, Loss: 0.1181, ACC: 5.014492988586426\n",
      "Epoch: 7310, Loss: 0.1181, ACC: 5.014492988586426\n",
      "Epoch: 7320, Loss: 0.1180, ACC: 5.014492988586426\n",
      "Epoch: 7330, Loss: 0.1180, ACC: 5.014492988586426\n",
      "Epoch: 7340, Loss: 0.1179, ACC: 5.014492988586426\n",
      "Epoch: 7350, Loss: 0.1179, ACC: 5.014492988586426\n",
      "Epoch: 7360, Loss: 0.1178, ACC: 5.014492988586426\n",
      "Epoch: 7370, Loss: 0.1178, ACC: 5.014492988586426\n",
      "Epoch: 7380, Loss: 0.1177, ACC: 5.014492988586426\n",
      "Epoch: 7390, Loss: 0.1177, ACC: 5.014492988586426\n",
      "Epoch: 7400, Loss: 0.1176, ACC: 5.014492988586426\n",
      "Epoch: 7410, Loss: 0.1176, ACC: 5.014492988586426\n",
      "Epoch: 7420, Loss: 0.1175, ACC: 5.014492988586426\n",
      "Epoch: 7430, Loss: 0.1175, ACC: 5.014492988586426\n",
      "Epoch: 7440, Loss: 0.1175, ACC: 5.014492988586426\n",
      "Epoch: 7450, Loss: 0.1174, ACC: 5.014492988586426\n",
      "Epoch: 7460, Loss: 0.1174, ACC: 5.014492988586426\n",
      "Epoch: 7470, Loss: 0.1173, ACC: 5.014492988586426\n",
      "Epoch: 7480, Loss: 0.1173, ACC: 5.014492988586426\n",
      "Epoch: 7490, Loss: 0.1172, ACC: 5.014492988586426\n",
      "Epoch: 7500, Loss: 0.1172, ACC: 5.014492988586426\n",
      "Epoch: 7510, Loss: 0.1172, ACC: 5.014492988586426\n",
      "Epoch: 7520, Loss: 0.1171, ACC: 5.014492988586426\n",
      "Epoch: 7530, Loss: 0.1171, ACC: 5.014492988586426\n",
      "Epoch: 7540, Loss: 0.1171, ACC: 5.014492988586426\n",
      "Epoch: 7550, Loss: 0.1170, ACC: 5.014492988586426\n",
      "Epoch: 7560, Loss: 0.1170, ACC: 5.014492988586426\n",
      "Epoch: 7570, Loss: 0.1169, ACC: 5.014492988586426\n",
      "Epoch: 7580, Loss: 0.1169, ACC: 5.014492988586426\n",
      "Epoch: 7590, Loss: 0.1169, ACC: 5.014492988586426\n",
      "Epoch: 7600, Loss: 0.1168, ACC: 5.014492988586426\n",
      "Epoch: 7610, Loss: 0.1168, ACC: 5.014492988586426\n",
      "Epoch: 7620, Loss: 0.1168, ACC: 5.014492988586426\n",
      "Epoch: 7630, Loss: 0.1167, ACC: 5.014492988586426\n",
      "Epoch: 7640, Loss: 0.1167, ACC: 5.014492988586426\n",
      "Epoch: 7650, Loss: 0.1167, ACC: 5.014492988586426\n",
      "Epoch: 7660, Loss: 0.1166, ACC: 5.014492988586426\n",
      "Epoch: 7670, Loss: 0.1166, ACC: 5.014492988586426\n",
      "Epoch: 7680, Loss: 0.1166, ACC: 5.014492988586426\n",
      "Epoch: 7690, Loss: 0.1165, ACC: 5.014492988586426\n",
      "Epoch: 7700, Loss: 0.1165, ACC: 5.014492988586426\n",
      "Epoch: 7710, Loss: 0.1165, ACC: 5.014492988586426\n",
      "Epoch: 7720, Loss: 0.1164, ACC: 5.014492988586426\n",
      "Epoch: 7730, Loss: 0.1164, ACC: 5.009661674499512\n",
      "Epoch: 7740, Loss: 0.1164, ACC: 5.009661674499512\n",
      "Epoch: 7750, Loss: 0.1164, ACC: 5.009661674499512\n",
      "Epoch: 7760, Loss: 0.1163, ACC: 5.009661674499512\n",
      "Epoch: 7770, Loss: 0.1163, ACC: 5.009661674499512\n",
      "Epoch: 7780, Loss: 0.1163, ACC: 5.009661674499512\n",
      "Epoch: 7790, Loss: 0.1162, ACC: 5.009661674499512\n",
      "Epoch: 7800, Loss: 0.1162, ACC: 5.009661674499512\n",
      "Epoch: 7810, Loss: 0.1162, ACC: 5.009661674499512\n",
      "Epoch: 7820, Loss: 0.1162, ACC: 5.009661674499512\n",
      "Epoch: 7830, Loss: 0.1161, ACC: 5.009661674499512\n",
      "Epoch: 7840, Loss: 0.1161, ACC: 5.009661674499512\n",
      "Epoch: 7850, Loss: 0.1161, ACC: 5.009661674499512\n",
      "Epoch: 7860, Loss: 0.1161, ACC: 5.009661674499512\n",
      "Epoch: 7870, Loss: 0.1160, ACC: 5.009661674499512\n",
      "Epoch: 7880, Loss: 0.1160, ACC: 5.009661674499512\n",
      "Epoch: 7890, Loss: 0.1160, ACC: 5.009661674499512\n",
      "Epoch: 7900, Loss: 0.1160, ACC: 5.009661674499512\n",
      "Epoch: 7910, Loss: 0.1159, ACC: 5.009661674499512\n",
      "Epoch: 7920, Loss: 0.1159, ACC: 5.009661674499512\n",
      "Epoch: 7930, Loss: 0.1159, ACC: 5.009661674499512\n",
      "Epoch: 7940, Loss: 0.1159, ACC: 5.009661674499512\n",
      "Epoch: 7950, Loss: 0.1159, ACC: 5.019323825836182\n",
      "Epoch: 7960, Loss: 0.1158, ACC: 5.019323825836182\n",
      "Epoch: 7970, Loss: 0.1158, ACC: 5.019323825836182\n",
      "Epoch: 7980, Loss: 0.1158, ACC: 5.019323825836182\n",
      "Epoch: 7990, Loss: 0.1158, ACC: 5.019323825836182\n",
      "Epoch: 8000, Loss: 0.1157, ACC: 5.019323825836182\n",
      "Epoch: 8010, Loss: 0.1157, ACC: 5.019323825836182\n",
      "Epoch: 8020, Loss: 0.1157, ACC: 5.019323825836182\n",
      "Epoch: 8030, Loss: 0.1157, ACC: 5.019323825836182\n",
      "Epoch: 8040, Loss: 0.1157, ACC: 5.019323825836182\n",
      "Epoch: 8050, Loss: 0.1156, ACC: 5.019323825836182\n",
      "Epoch: 8060, Loss: 0.1156, ACC: 5.019323825836182\n",
      "Epoch: 8070, Loss: 0.1156, ACC: 5.028985500335693\n",
      "Epoch: 8080, Loss: 0.1156, ACC: 5.028985500335693\n",
      "Epoch: 8090, Loss: 0.1156, ACC: 5.028985500335693\n",
      "Epoch: 8100, Loss: 0.1155, ACC: 5.028985500335693\n",
      "Epoch: 8110, Loss: 0.1155, ACC: 5.028985500335693\n",
      "Epoch: 8120, Loss: 0.1155, ACC: 5.028985500335693\n",
      "Epoch: 8130, Loss: 0.1155, ACC: 5.028985500335693\n",
      "Epoch: 8140, Loss: 0.1155, ACC: 5.028985500335693\n",
      "Epoch: 8150, Loss: 0.1155, ACC: 5.028985500335693\n",
      "Epoch: 8160, Loss: 0.1154, ACC: 5.028985500335693\n",
      "Epoch: 8170, Loss: 0.1154, ACC: 5.028985500335693\n",
      "Epoch: 8180, Loss: 0.1154, ACC: 5.028985500335693\n",
      "Epoch: 8190, Loss: 0.1154, ACC: 5.028985500335693\n",
      "Epoch: 8200, Loss: 0.1154, ACC: 5.028985500335693\n",
      "Epoch: 8210, Loss: 0.1153, ACC: 5.028985500335693\n",
      "Epoch: 8220, Loss: 0.1153, ACC: 5.028985500335693\n",
      "Epoch: 8230, Loss: 0.1153, ACC: 5.028985500335693\n",
      "Epoch: 8240, Loss: 0.1153, ACC: 5.028985500335693\n",
      "Epoch: 8250, Loss: 0.1153, ACC: 5.033816337585449\n",
      "Epoch: 8260, Loss: 0.1153, ACC: 5.033816337585449\n",
      "Epoch: 8270, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8280, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8290, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8300, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8310, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8320, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8330, Loss: 0.1152, ACC: 5.033816337585449\n",
      "Epoch: 8340, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8350, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8360, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8370, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8380, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8390, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8400, Loss: 0.1151, ACC: 5.033816337585449\n",
      "Epoch: 8410, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8420, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8430, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8440, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8450, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8460, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8470, Loss: 0.1150, ACC: 5.033816337585449\n",
      "Epoch: 8480, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8490, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8500, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8510, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8520, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8530, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8540, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8550, Loss: 0.1149, ACC: 5.033816337585449\n",
      "Epoch: 8560, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8570, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8580, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8590, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8600, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8610, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8620, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8630, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8640, Loss: 0.1148, ACC: 5.033816337585449\n",
      "Epoch: 8650, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8660, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8670, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8680, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8690, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8700, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8710, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8720, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8730, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8740, Loss: 0.1147, ACC: 5.033816337585449\n",
      "Epoch: 8750, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8760, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8770, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8780, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8790, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8800, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8810, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8820, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8830, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8840, Loss: 0.1146, ACC: 5.033816337585449\n",
      "Epoch: 8850, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8860, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8870, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8880, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8890, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8900, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8910, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8920, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8930, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8940, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8950, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8960, Loss: 0.1145, ACC: 5.033816337585449\n",
      "Epoch: 8970, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 8980, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 8990, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9000, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9010, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9020, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9030, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9040, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9050, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9060, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9070, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9080, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9090, Loss: 0.1144, ACC: 5.033816337585449\n",
      "Epoch: 9100, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9110, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9120, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9130, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9140, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9150, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9160, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9170, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9180, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9190, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9200, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9210, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9220, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9230, Loss: 0.1143, ACC: 5.033816337585449\n",
      "Epoch: 9240, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9250, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9260, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9270, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9280, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9290, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9300, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9310, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9320, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9330, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9340, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9350, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9360, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9370, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9380, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9390, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9400, Loss: 0.1142, ACC: 5.033816337585449\n",
      "Epoch: 9410, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9420, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9430, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9440, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9450, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9460, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9470, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9480, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9490, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9500, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9510, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9520, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9530, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9540, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9550, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9560, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9570, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9580, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9590, Loss: 0.1141, ACC: 5.033816337585449\n",
      "Epoch: 9600, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9610, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9620, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9630, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9640, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9650, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9660, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9670, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9680, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9690, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9700, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9710, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9720, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9730, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9740, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9750, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9760, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9770, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9780, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9790, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9800, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9810, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9820, Loss: 0.1140, ACC: 5.033816337585449\n",
      "Epoch: 9830, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9840, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9850, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9860, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9870, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9880, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9890, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9900, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9910, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9920, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9930, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9940, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9950, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9960, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9970, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9980, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 9990, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10000, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10010, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10020, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10030, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10040, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10050, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10060, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10070, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10080, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10090, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10100, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10110, Loss: 0.1139, ACC: 5.033816337585449\n",
      "Epoch: 10120, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10130, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10140, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10150, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10160, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10170, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10180, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10190, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10200, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10210, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10220, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10230, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10240, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10250, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10260, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10270, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10280, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10290, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10300, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10310, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10320, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10330, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10340, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10350, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10360, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10370, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10380, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10390, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10400, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10410, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10420, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10430, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10440, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10450, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10460, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10470, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10480, Loss: 0.1138, ACC: 5.033816337585449\n",
      "Epoch: 10490, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10500, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10510, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10520, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10530, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10540, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10550, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10560, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10570, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10580, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10590, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10600, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10610, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10620, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10630, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10640, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10650, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10660, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10670, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10680, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10690, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10700, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10710, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10720, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10730, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10740, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10750, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10760, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10770, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10780, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10790, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10800, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10810, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10820, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10830, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10840, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10850, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10860, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10870, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10880, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10890, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10900, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10910, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10920, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10930, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10940, Loss: 0.1137, ACC: 5.033816337585449\n",
      "Epoch: 10950, Loss: 0.1137, ACC: 5.038647174835205\n",
      "Epoch: 10960, Loss: 0.1137, ACC: 5.038647174835205\n",
      "Epoch: 10970, Loss: 0.1137, ACC: 5.038647174835205\n",
      "Epoch: 10980, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 10990, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11000, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11010, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11020, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11030, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11040, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11050, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11060, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11070, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11080, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11090, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11100, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11110, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11120, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11130, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11140, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11150, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11160, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11170, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11180, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11190, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11200, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11210, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11220, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11230, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11240, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11250, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11260, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11270, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11280, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11290, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11300, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11310, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11320, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11330, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11340, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11350, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11360, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11370, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11380, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11390, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11400, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11410, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11420, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11430, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11440, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11450, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11460, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11470, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11480, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11490, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11500, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11510, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11520, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11530, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11540, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11550, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11560, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11570, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11580, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11590, Loss: 0.1136, ACC: 5.038647174835205\n",
      "Epoch: 11600, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11610, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11620, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11630, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11640, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11650, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11660, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11670, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11680, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11690, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11700, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11710, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11720, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11730, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11740, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11750, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11760, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11770, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11780, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11790, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11800, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11810, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11820, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11830, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11840, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11850, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11860, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11870, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11880, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11890, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11900, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11910, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11920, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11930, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11940, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11950, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11960, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11970, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11980, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 11990, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12000, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12010, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12020, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12030, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12040, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12050, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12060, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12070, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12080, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12090, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12100, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12110, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12120, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12130, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12140, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12150, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12160, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12170, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12180, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12190, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12200, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12210, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12220, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12230, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12240, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12250, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12260, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12270, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12280, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12290, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12300, Loss: 0.1135, ACC: 5.038647174835205\n",
      "Epoch: 12310, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12320, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12330, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12340, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12350, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12360, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12370, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12380, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12390, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12400, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12410, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12420, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12430, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12440, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12450, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12460, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12470, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12480, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12490, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12500, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12510, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12520, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12530, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12540, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12550, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12560, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12570, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12580, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12590, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12600, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12610, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12620, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12630, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12640, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12650, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12660, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12670, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12680, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12690, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12700, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12710, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12720, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12730, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12740, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12750, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12760, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12770, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12780, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12790, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12800, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12810, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12820, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12830, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12840, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12850, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12860, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12870, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12880, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12890, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12900, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12910, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12920, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12930, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12940, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12950, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12960, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12970, Loss: 0.1134, ACC: 5.038647174835205\n",
      "Epoch: 12980, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 12990, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13000, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13010, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13020, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13030, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13040, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13050, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13060, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13070, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13080, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13090, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13100, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13110, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13120, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13130, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13140, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13150, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13160, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13170, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13180, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13190, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13200, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13210, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13220, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13230, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13240, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13250, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13260, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13270, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13280, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13290, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13300, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13310, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13320, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13330, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13340, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13350, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13360, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13370, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13380, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13390, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13400, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13410, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13420, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13430, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13440, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13450, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13460, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13470, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13480, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13490, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13500, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13510, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13520, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13530, Loss: 0.1133, ACC: 5.038647174835205\n",
      "Epoch: 13540, Loss: 0.1132, ACC: 5.038647174835205\n",
      "Epoch: 13550, Loss: 0.1132, ACC: 5.038647174835205\n",
      "Epoch: 13560, Loss: 0.1132, ACC: 5.038647174835205\n",
      "Epoch: 13570, Loss: 0.1132, ACC: 5.038647174835205\n",
      "Epoch: 13580, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13590, Loss: 0.1132, ACC: 5.038647174835205\n",
      "Epoch: 13600, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13610, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13620, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13630, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13640, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13650, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13660, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13670, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13680, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13690, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13700, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13710, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13720, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13730, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13740, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13750, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13760, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13770, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13780, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13790, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13800, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13810, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13820, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13830, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13840, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13850, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13860, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13870, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13880, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13890, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13900, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13910, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13920, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13930, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13940, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13950, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13960, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13970, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13980, Loss: 0.1132, ACC: 5.041062831878662\n",
      "Epoch: 13990, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14000, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14010, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14020, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14030, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14040, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14050, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14060, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14070, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14080, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14090, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14100, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14110, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14120, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14130, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14140, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14150, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14160, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14170, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14180, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14190, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14200, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14210, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14220, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14230, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14240, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14250, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14260, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14270, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14280, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14290, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14300, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14310, Loss: 0.1131, ACC: 5.041062831878662\n",
      "Epoch: 14320, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14330, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14340, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14350, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14360, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14370, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14380, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14390, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14400, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14410, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14420, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14430, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14440, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14450, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14460, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14470, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14480, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14490, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14500, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14510, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14520, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14530, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14540, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14550, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14560, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14570, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14580, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14590, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14600, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14610, Loss: 0.1130, ACC: 5.041062831878662\n",
      "Epoch: 14620, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14630, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14640, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14650, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14660, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14670, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14680, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14690, Loss: 0.1129, ACC: 5.041062831878662\n",
      "Epoch: 14700, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14710, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14720, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14730, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14740, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14750, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14760, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14770, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14780, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14790, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14800, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14810, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14820, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14830, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14840, Loss: 0.1129, ACC: 5.048309326171875\n",
      "Epoch: 14850, Loss: 0.1128, ACC: 5.048309326171875\n",
      "Epoch: 14860, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14870, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14880, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14890, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14900, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14910, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14920, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14930, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14940, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14950, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14960, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14970, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14980, Loss: 0.1128, ACC: 5.043478488922119\n",
      "Epoch: 14990, Loss: 0.1127, ACC: 5.043478488922119\n",
      "Epoch: 15000, Loss: 0.1127, ACC: 5.043478488922119\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "acc_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    targets_preds = model(inputs_train)\n",
    "    loss = loss_fn(targets_preds,targets_train)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch+1)%10 == 0:\n",
    "        with torch.no_grad():\n",
    "            targets_preds = model(inputs_test)\n",
    "            targets_preds_cls = targets_preds.round()\n",
    "            acc = targets_preds_cls.eq(targets_test).sum() / float(inputs_test.shape[0])\n",
    "            loss_hist.append(loss.item())\n",
    "            acc_hist.append(acc)\n",
    "            print(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}, ACC: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model/final_crime_lats.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist,label=\"Loss\")\n",
    "plt.plot(acc_hist,label=\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3500, 0.0452, 0.2081, 0.2549, 0.1095, 0.0319],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1987,  0.0238,  0.1915,  0.5279,  0.0379, -0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1594, -0.0308,  0.4274,  0.3492,  0.0297,  0.0262],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.0411, 0.4151, 0.3462, 0.0269, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0860,  0.2663,  0.2033,  0.0869, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3500, 0.0452, 0.2081, 0.2549, 0.1095, 0.0319],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3673, 0.0708, 0.2453, 0.2257, 0.0920, 0.0023],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1822, 0.0371, 0.4306, 0.2917, 0.0340, 0.0200],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1822, 0.0371, 0.4306, 0.2917, 0.0340, 0.0200],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1594, -0.0308,  0.4274,  0.3492,  0.0297,  0.0262],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.1308, 0.1692, 0.5222, 0.0386, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.1308, 0.1692, 0.5222, 0.0386, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.1308, 0.1692, 0.5222, 0.0386, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1594, -0.0308,  0.4274,  0.3492,  0.0297,  0.0262],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3673, 0.0708, 0.2453, 0.2257, 0.0920, 0.0023],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1822, 0.0371, 0.4306, 0.2917, 0.0340, 0.0200],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1594, -0.0308,  0.4274,  0.3492,  0.0297,  0.0262],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0355,  0.3820,  0.3657,  0.0760, -0.0093],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.0411, 0.4151, 0.3462, 0.0269, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1987,  0.0238,  0.1915,  0.5279,  0.0379, -0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0355,  0.3820,  0.3657,  0.0760, -0.0093],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0355,  0.3820,  0.3657,  0.0760, -0.0093],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0355,  0.3820,  0.3657,  0.0760, -0.0093],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3500, 0.0452, 0.2081, 0.2549, 0.1095, 0.0319],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0860,  0.2663,  0.2033,  0.0869, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3500, 0.0452, 0.2081, 0.2549, 0.1095, 0.0319],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3673, 0.0708, 0.2453, 0.2257, 0.0920, 0.0023],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1594, -0.0308,  0.4274,  0.3492,  0.0297,  0.0262],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3673, 0.0708, 0.2453, 0.2257, 0.0920, 0.0023],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0860,  0.2663,  0.2033,  0.0869, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0355,  0.3820,  0.3657,  0.0760, -0.0093],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3485, 0.0448, 0.2064, 0.2539, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1822, 0.0371, 0.4306, 0.2917, 0.0340, 0.0200],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3673, 0.0708, 0.2453, 0.2257, 0.0920, 0.0023],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.0411, 0.4151, 0.3462, 0.0269, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3500, 0.0452, 0.2081, 0.2549, 0.1095, 0.0319],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3500, 0.0452, 0.2081, 0.2549, 0.1095, 0.0319],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.0411, 0.4151, 0.3462, 0.0269, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1987,  0.0238,  0.1915,  0.5279,  0.0379, -0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1987,  0.0238,  0.1915,  0.5279,  0.0379, -0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1822, 0.0371, 0.4306, 0.2917, 0.0340, 0.0200],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.1308, 0.1692, 0.5222, 0.0386, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1271, -0.0247,  0.4264,  0.3284,  0.0402,  0.0090],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2533,  0.0365,  0.3252,  0.3235,  0.1461, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1636,  0.0208,  0.3852,  0.3666,  0.0761, -0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1995,  0.0244,  0.1938,  0.5287,  0.0347, -0.0201],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1686,  0.0066,  0.5173,  0.2696, -0.0496,  0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1859, -0.0728,  0.4382,  0.3521,  0.0264,  0.0152],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1751, -0.0084,  0.4263,  0.3672,  0.0182,  0.0496],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1987,  0.0238,  0.1915,  0.5279,  0.0379, -0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5151e-01, 1.0207e-01, 1.7580e-01, 5.2395e-01, 3.7976e-02, 2.0596e-04],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3631,  0.0735,  0.2437,  0.2167,  0.0991, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1221, 0.0348, 0.4186, 0.3472, 0.0237, 0.0432],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1239, 0.1726, 0.5232, 0.0363, 0.0059],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3617, 0.0689, 0.2397, 0.2220, 0.1005, 0.0032],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2004, 0.0342, 0.3784, 0.3008, 0.0895, 0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1822, 0.0371, 0.4306, 0.2917, 0.0340, 0.0200],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.0411, 0.4151, 0.3462, 0.0269, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1420, 0.1162, 0.1695, 0.5221, 0.0417, 0.0035],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1167, 0.1588, 0.1628, 0.5205, 0.0399, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2024, 0.0369, 0.4006, 0.3008, 0.0659, 0.0117],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1720, 0.0372, 0.4457, 0.2872, 0.0180, 0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.0037, 0.4073, 0.3431, 0.0445, 0.0341],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1928, -0.0364,  0.4336,  0.3692,  0.0158,  0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2432,  0.0366,  0.3402,  0.3190,  0.1301, -0.0051],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1211, 0.1518, 0.1647, 0.5210, 0.0393, 0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2115,  0.0026,  0.1950,  0.5288,  0.0384, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1265, 0.0279, 0.4205, 0.3478, 0.0230, 0.0414],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1698, -0.0963,  0.4401,  0.3318,  0.0412, -0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1894, -0.0215,  0.3936,  0.3687,  0.0757, -0.0242],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1392, 0.0061, 0.4242, 0.3487, 0.0241, 0.0358],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2215, -0.0763,  0.4511,  0.3743,  0.0032,  0.0318],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2976,  0.0603,  0.3638,  0.2735,  0.1382, -0.0485],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1808, -0.0075,  0.3908,  0.3680,  0.0758, -0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1566, -0.0221,  0.4305,  0.3503,  0.0231,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2730,  0.0363,  0.2967,  0.3332,  0.1755, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0888, 0.0393, 0.4150, 0.3256, 0.0390, 0.0259],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2337,  0.0368,  0.3550,  0.3144,  0.1148, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1262, 0.1455, 0.1686, 0.5222, 0.0358, 0.0114],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3980,  0.1301,  0.3260,  0.1445,  0.0672, -0.0697],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.1308, 0.1692, 0.5222, 0.0386, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1681,  0.0142,  0.3871,  0.3671,  0.0749, -0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1309, 0.0095, 0.4102, 0.3442, 0.0415, 0.0366],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3587, 0.0613, 0.2300, 0.2336, 0.1022, 0.0126],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1514, -0.0157,  0.4262,  0.3490,  0.0270,  0.0300],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1734, 0.0375, 0.4467, 0.2878, 0.0169, 0.0237],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1303, 0.1379, 0.1694, 0.5224, 0.0370, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1642,  0.0810,  0.1793,  0.5248,  0.0384, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1559,  0.0953,  0.1774,  0.5244,  0.0374, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1772,  0.0594,  0.1842,  0.5261,  0.0380, -0.0110],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1644,  0.0811,  0.1799,  0.5250,  0.0377, -0.0054],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1438,  0.0572,  0.3839,  0.3667,  0.0691, -0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1902, -0.0797,  0.4398,  0.3525,  0.0260,  0.0133],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1724,  0.0071,  0.3886,  0.3675,  0.0749, -0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3662,  0.0769,  0.2508,  0.2115,  0.0966, -0.0065],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0897,  0.2692,  0.1968,  0.0880, -0.0213],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0975, 0.0254, 0.4177, 0.3263, 0.0387, 0.0221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1464,  0.0494,  0.3795,  0.3651,  0.0759, -0.0055],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3651, 0.0667, 0.2397, 0.2310, 0.0940, 0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2186, -0.0787,  0.4421,  0.3714,  0.0153,  0.0311],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1608,  0.0117,  0.5061,  0.2650, -0.0348,  0.0228],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1585, 0.0207, 0.4221, 0.3663, 0.0158, 0.0572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1191, -0.0101,  0.4254,  0.3283,  0.0378,  0.0129],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2162, -0.1221,  0.4490,  0.3549,  0.0250,  0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1770,  0.0596,  0.1831,  0.5257,  0.0390, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1129, 0.1665, 0.1627, 0.5206, 0.0379, 0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3811,  0.1015,  0.2858,  0.1807,  0.0826, -0.0357],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1797, -0.0085,  0.3876,  0.3669,  0.0804, -0.0206],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1479, 0.1102, 0.1761, 0.5241, 0.0350, 0.0021],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3561, 0.0570, 0.2242, 0.2386, 0.1047, 0.0175],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1779, -0.0073,  0.5162,  0.2625, -0.0396,  0.0131],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3642,  0.0730,  0.2455,  0.2169,  0.0981, -0.0017],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1103, 0.0038, 0.4219, 0.3273, 0.0390, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1557, -0.0228,  0.4276,  0.3494,  0.0270,  0.0281],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2281, 0.0968, 0.4112, 0.5039, 0.1229, 0.0816],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0860,  0.2663,  0.2033,  0.0869, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0355,  0.3820,  0.3657,  0.0760, -0.0093],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1135, 0.1668, 0.1650, 0.5214, 0.0352, 0.0171],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2649,  0.0367,  0.3108,  0.3286,  0.1614, -0.0138],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3702,  0.0780,  0.2551,  0.2139,  0.0908, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([1.5111e-01, 1.0197e-01, 1.7435e-01, 5.2348e-01, 3.9590e-02, 4.5560e-05],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2072,  0.0097,  0.1936,  0.5284,  0.0384, -0.0241],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1277, -0.0243,  0.4281,  0.3290,  0.0380,  0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1342, 0.1308, 0.1692, 0.5222, 0.0386, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1683,  0.0734,  0.1803,  0.5250,  0.0394, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3724,  0.0856,  0.2637,  0.2021,  0.0900, -0.0164],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1601, -0.0301,  0.4295,  0.3499,  0.0267,  0.0263],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1972, -0.0435,  0.4354,  0.3697,  0.0154,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2750,  0.0366,  0.2958,  0.3331,  0.1774, -0.0179],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1923, 0.0370, 0.4156, 0.2963, 0.0499, 0.0158],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1304, 0.1385, 0.1696, 0.5224, 0.0361, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1509,  0.0429,  0.3813,  0.3656,  0.0749, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1171, 0.1588, 0.1641, 0.5210, 0.0385, 0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2032, 0.0370, 0.3970, 0.2994, 0.0715, 0.0102],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1518,  0.0374,  0.4758,  0.2781, -0.0139,  0.0324],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1348, 0.0132, 0.4225, 0.3482, 0.0244, 0.0376],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1147, -0.0031,  0.4235,  0.3277,  0.0385,  0.0147],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1507,  0.0427,  0.3805,  0.3653,  0.0759, -0.0074],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3597, 0.0650, 0.2342, 0.2275, 0.1020, 0.0080],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1550,  0.0353,  0.3823,  0.3658,  0.0758, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.1591, 0.1632, 0.5207, 0.0391, 0.0149],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1245, 0.0373, 0.4258, 0.3496, 0.0134, 0.0437],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1640, 0.0145, 0.4276, 0.3680, 0.0106, 0.0557],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2034, -0.1008,  0.4450,  0.3539,  0.0247,  0.0078],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1755, -0.0078,  0.4274,  0.3676,  0.0163,  0.0497],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1645, 0.0050, 0.4164, 0.3642, 0.0263, 0.0526],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1710, -0.0010,  0.4253,  0.3670,  0.0173,  0.0515],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1475, 0.1099, 0.1748, 0.5237, 0.0368, 0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1595,  0.0284,  0.3842,  0.3663,  0.0751, -0.0111],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0842, 0.0463, 0.4125, 0.3249, 0.0402, 0.0277],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3680,  0.0740,  0.2495,  0.2192,  0.0927, -0.0022],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2369,  0.0354,  0.3424,  0.3190,  0.1252, -0.0013],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1594, -0.0308,  0.4274,  0.3492,  0.0297,  0.0262],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1344, 0.0134, 0.4208, 0.3477, 0.0260, 0.0374],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3623,  0.0836,  0.2523,  0.1963,  0.1039, -0.0151],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1551,  0.0356,  0.3824,  0.3659,  0.0754, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1163, 0.1585, 0.1614, 0.5201, 0.0417, 0.0148],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1330, 0.0117, 0.4167, 0.3463, 0.0322, 0.0371],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3709,  0.0851,  0.2621,  0.2011,  0.0923, -0.0162],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3490, 0.0449, 0.2070, 0.2543, 0.1110, 0.0320],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0352,  0.3820,  0.3657,  0.0762, -0.0092],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2137, 0.0371, 0.3870, 0.3061, 0.0800, 0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1734, -0.0513,  0.4355,  0.3515,  0.0246,  0.0209],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2114,  0.0026,  0.1946,  0.5287,  0.0387, -0.0260],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3985,  0.1336,  0.3298,  0.1381,  0.0678, -0.0742],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3545, 0.0534, 0.2190, 0.2444, 0.1056, 0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2629,  0.0364,  0.3117,  0.3286,  0.1595, -0.0125],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1586, 0.0206, 0.4228, 0.3665, 0.0152, 0.0573],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1840, -0.0153,  0.3890,  0.3673,  0.0801, -0.0226],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2412, 0.1484, 0.4183, 0.5584, 0.1415, 0.1256],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1540, 0.0276, 0.4202, 0.3658, 0.0166, 0.0590],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3573, 0.0611, 0.2284, 0.2328, 0.1040, 0.0128],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1381, 0.1687, 0.5221, 0.0375, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1643, -0.0370,  0.4305,  0.3501,  0.0269,  0.0244],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1974, -0.0433,  0.4358,  0.3698,  0.0147,  0.0405],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1390, 0.1243, 0.1725, 0.5232, 0.0360, 0.0058],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3626, 0.0693, 0.2405, 0.2227, 0.0990, 0.0030],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1258, 0.1452, 0.1672, 0.5218, 0.0375, 0.0113],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1711,  0.0057,  0.3847,  0.3662,  0.0805, -0.0169],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1469, 0.1094, 0.1730, 0.5231, 0.0392, 0.0019],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1833, -0.0123,  0.4399,  0.3716,  0.0016,  0.0488],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2441,  0.0368,  0.3417,  0.3197,  0.1281, -0.0050],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1623, 0.0132, 0.4222, 0.3662, 0.0177, 0.0552],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3484, 0.0449, 0.2063, 0.2540, 0.1118, 0.0321],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1901, -0.0210,  0.3962,  0.3695,  0.0724, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1307, 0.1387, 0.1703, 0.5227, 0.0351, 0.0096],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1853, -0.0145,  0.3932,  0.3686,  0.0748, -0.0222],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1020, 0.0185, 0.4200, 0.3269, 0.0376, 0.0204],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0223,  0.4292,  0.3499,  0.0247,  0.0282],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1979, -0.0350,  0.3965,  0.3700,  0.0759, -0.0273],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1294, 0.0193, 0.4178, 0.3468, 0.0291, 0.0392],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1654,  0.0381,  0.4651,  0.2849, -0.0036,  0.0279],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1296, 0.0300, 0.4303, 0.3510, 0.0104, 0.0423],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3673, 0.0708, 0.2453, 0.2257, 0.0920, 0.0023],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1016, 0.0180, 0.4186, 0.3265, 0.0395, 0.0203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1169, 0.0409, 0.4144, 0.3460, 0.0278, 0.0449],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1106, 0.0042, 0.4228, 0.3276, 0.0376, 0.0166],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1685,  0.0736,  0.1808,  0.5252,  0.0387, -0.0073],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0284,  0.3834,  0.3661,  0.0759, -0.0112],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1318, -0.0316,  0.4290,  0.3291,  0.0387,  0.0072],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1562, -0.0228,  0.4298,  0.3501,  0.0247,  0.0284],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2438,  0.0368,  0.3417,  0.3197,  0.1280, -0.0049],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1549,  0.0948,  0.1738,  0.5232,  0.0416, -0.0020],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2077,  0.0098,  0.1954,  0.5290,  0.0365, -0.0239],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1810, 0.0369, 0.4311, 0.2918, 0.0330, 0.0207],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2139, -0.0722,  0.4394,  0.3706,  0.0174,  0.0329],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1593,  0.0873,  0.1759,  0.5238,  0.0415, -0.0038],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1523, 0.1656, 0.5213, 0.0378, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.0882, 0.0389, 0.4127, 0.3248, 0.0418, 0.0257],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1516,  0.0432,  0.3837,  0.3664,  0.0719, -0.0071],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1633,  0.0207,  0.3844,  0.3663,  0.0770, -0.0130],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1923, -0.0185,  0.4025,  0.3716,  0.0629, -0.0235],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3649,  0.0733,  0.2461,  0.2174,  0.0970, -0.0018],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1215, 0.1521, 0.1656, 0.5214, 0.0380, 0.0132],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.3580, 0.0611, 0.2292, 0.2331, 0.1032, 0.0127],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.3747,  0.0860,  0.2663,  0.2033,  0.0869, -0.0167],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1582, 0.0206, 0.4211, 0.3660, 0.0169, 0.0571],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1301, 0.1380, 0.1687, 0.5221, 0.0376, 0.0095],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 0.1679,  0.0731,  0.1790,  0.5246,  0.0412, -0.0075],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.1343, 0.0126, 0.4211, 0.3478, 0.0266, 0.0375],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in inputs_train:\n",
    "    pred = model(i)\n",
    "    print(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
